{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPj3TQkUZihZ"
   },
   "source": [
    "# **Text Vectorization I**\n",
    "\n",
    "---\n",
    "### Natural Language Processing\n",
    "Date: Nov 11, 2022\n",
    "\n",
    "Authors: Jerónimo Arenas-García (jarenas@ing.uc3m.es), Lorena Calvo-Bartolomé (lcalvo@pa.uc3m.es), Jesús Cid-Suero (jcid@ing.uc3m.es)\n",
    "\n",
    "Version 1.0\n",
    "\n",
    "---\n",
    "\n",
    "Our goal here is to provide a basic overview of the following aspects:\n",
    "\n",
    "\n",
    "*   NLP preprocessing\n",
    "*   Document BoW and TF-IDF representation\n",
    "*   Utilization of the latter to solve a Text Classification task\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22608,
     "status": "ok",
     "timestamp": 1732297702591,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "hTPdyX5-bb-w",
    "outputId": "811a0d20-90da-478d-b7ca-5ed1ff6d07a2",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Google Drive integration\n",
    "\n",
    "# Libraries to work with Google Drive and the file system\n",
    "from google.colab import drive\n",
    "import os, sys\n",
    "\n",
    "# Drive is mounted\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Current directory is shown\n",
    "print(os.getcwd())\n",
    "\n",
    "# We change to work directory\n",
    "directory_path = \"/content/drive/MyDrive/Colab Notebooks/proyecto_td/reference_code/Text_Vectorization_I/\"  # Define directory_path here\n",
    "if not os.path.exists(directory_path):\n",
    "  os.makedirs(directory_path)\n",
    "  print(f\"Directory created: {directory_path}\")\n",
    "os.chdir(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIEhJkSaWW8Z"
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Figures plotted inside the notebook\n",
    "%matplotlib inline\n",
    "# High quality figures\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "# Figures style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.color_palette(\"deep\")\n",
    "# Figues size\n",
    "plt.rcParams['figure.figsize'] = [8, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s33bt1hoFviC"
   },
   "outputs": [],
   "source": [
    "# To wrap long text lines\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1732297712417,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "D-LnZYfyY9EX",
    "outputId": "58a812e3-7761-48cd-b10f-b2ad46b5d680"
   },
   "outputs": [],
   "source": [
    "# For fancy table Display\n",
    "%load_ext google.colab.data_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "myiD58wpbymQ"
   },
   "source": [
    "## **1. Data preparation**\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZ3x5vfLclE_"
   },
   "source": [
    "### *1.1. Data loading*\n",
    "\n",
    "The first step to start working with text vectorization is downloading the dataset with which we will work. Here we will be using the **IMDB Dataset of 50K Movie Reviews**, which contains 50K movie reviews for natural language processing or Text analytics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "executionInfo": {
     "elapsed": 6718,
     "status": "ok",
     "timestamp": 1732297719123,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "P2G-hmU3b24N",
    "outputId": "841c3db9-0cbc-423e-cde8-6daffcfc340a"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  import opendatasets as od\n",
    "except ModuleNotFoundError:\n",
    "  %pip install opendatasets\n",
    "  import opendatasets as od"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PudNbhBg-uAi"
   },
   "source": [
    "The IMDB dataset can be downloaded from **Kaggle**. For doing so, you must create an account at [kaggle.com](https://www.kaggle.com/).\n",
    "\n",
    "Once you have your account, go to ``Your profile`` and select ``Edit Public Profile``. If you scroll down in this view, you will see a button named ``Create New API Token``. By clicking it will automatically download a file ``kaggle.json`` containing your **username** and **key**. You will need them for executing the next cell.\n",
    "\n",
    "You just need to execute the following cell once, since it will store the dataset file in the drive folder you have specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "executionInfo": {
     "elapsed": 12730,
     "status": "ok",
     "timestamp": 1732297731848,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "uAkQE-SP5_pK",
    "outputId": "92a3987a-1d7f-4cc8-9824-de1200fb7eea"
   },
   "outputs": [],
   "source": [
    "# TODO: Comment this cell after executing it the first time\n",
    "od.download(\"https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSWQtKh4_b4v"
   },
   "source": [
    "Let's save the dataset as dataframe.\n",
    "\n",
    "This dataset is oriented toward binary sentence classification, i.e., the prediction of whether each of the reviews in the dataset is positive or negative using either classification or deep learning algorithms.\n",
    "\n",
    "Hence, we have two columns in our dataframe: the ``review`` column contains the textual information and the ``sentiment`` column contains the output labels. Here we will be working first on the ``review`` column for applying distinct types of text vectorization, and finally, we will utilize the ``sentiment`` column to carry out the sentiment analysis task.\n",
    "\n",
    "To accelerate the process, we will be only using a third of the reviews contained in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1270
    },
    "executionInfo": {
     "elapsed": 2886,
     "status": "ok",
     "timestamp": 1732297734722,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "dvre6LFf8puM",
    "outputId": "9ccc1624-0c2e-42a0-da55-6623b7cc597b"
   },
   "outputs": [],
   "source": [
    "corpus_df = pd.read_csv('imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv')\n",
    "corpus_df = corpus_df.sample(frac=0.3, replace=True, random_state=1)\n",
    "print(len(corpus_df))\n",
    "corpus_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vMI2q90cnLY"
   },
   "source": [
    "### *1.2. Preprocessing*\n",
    "\n",
    "Before continuing with the vectorization task, we should structure and clean the text so that we only keep the information that allows us to capture the semantic content of the corpus. This will improve the result of our embeddings.\n",
    "\n",
    "For this purpose, we will apply the following three steps, which are typical of any NLP processing task:\n",
    "\n",
    "1.   Text Wrangling\n",
    "2.   Tokenization\n",
    "3.   Homogenization\n",
    "4.   Cleaning\n",
    "\n",
    "For the next steps, we will be using some methods available from:\n",
    "\n",
    "*   [Natural Language Toolkit](https://www.nltk.org/)\n",
    "*   [Beautiful Soup](https://pypi.org/project/beautifulsoup4/)\n",
    "*   [Contractions](https://pypi.org/project/contractions/)\n",
    "*   [re — Regular expression operations](https://docs.python.org/3/library/re.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "executionInfo": {
     "elapsed": 21037,
     "status": "ok",
     "timestamp": 1732297755755,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "1dq2D5uzfHaR",
    "outputId": "ed5be6f0-1cca-47ce-bec7-8c723f468f12"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "def check_nltk_packages():\n",
    "  packages = ['punkt','stopwords','omw-1.4','wordnet']\n",
    "\n",
    "  for package in packages:\n",
    "    try:\n",
    "      nltk.data.find('tokenizers/' + package)\n",
    "    except LookupError:\n",
    "      nltk.download(package)\n",
    "check_nltk_packages()\n",
    "\n",
    "#nltk.download('punk_tab')\n",
    "\n",
    "try:\n",
    "  import lxml\n",
    "except ModuleNotFoundError:\n",
    "  %pip install lxml\n",
    "\n",
    "try:\n",
    "  import contractions\n",
    "except ModuleNotFoundError:\n",
    "  %pip install contractions\n",
    "  import contractions\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1732298456696,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "1zCIPfOo_ov4",
    "outputId": "255edbb9-c492-4281-e33d-c661fc0be130"
   },
   "outputs": [],
   "source": [
    "nltk.download('punk_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DKEcMr5aMcu"
   },
   "source": [
    "#### *1.2.1. Text Wrangling*\n",
    "\n",
    "If we inspect the reviews, we can see that they contain many HTML tags and some URLs that we do not want to keep for our text vectorization task since they don't add much value for understanding and analyzing the text.\n",
    "\n",
    "Additionally, there are many English contractions ('ll, 're) that we would like to transform into their base form (will, are) to later help with the standardization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUpFZpQBE9Cl"
   },
   "source": [
    "##### **Exercise 1**\n",
    "\n",
    "Complete the function ``wrangle_text`` that performs the text wrangling task. For doing so:\n",
    "\n",
    "*   Make use of the library ``BeautifulSoup`` with the parser ``\"lxml\"`` to get rid of all HTML tags.\n",
    "*   Use the function ``re.sub`` to remove all URLs in text. To this function, we need to provide a **regular expression**, i.e., a special sequence of characters that help us match or find other strings or sets of strings, using a specialized syntax held in a pattern, and a string to replace the occurrences of the regular expression found. Typically, we would use define the regular expressions as raw strings in the form r'expression'.\n",
    "You can identify URLs using the pattern ``r'https://\\S+|www\\.\\S+'``.\n",
    "* Use the method ``fix`` from the ``contractions`` library to expand the contractions.\n",
    "\n",
    "Apply the ``wrangle_text`` function into the first positive ``review`` in the corpus and save the result into a variable named ``wrangled_review``. Print the review before and after making the text wrangling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1732297794383,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "8s7hUSGrzKVm",
    "outputId": "adc66d2e-1f90-4532-edac-6979c2f93e8e"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "\n",
    "def wrangle_text(text):\n",
    "  # Get rid of all HTML tags\n",
    "  soup = BeautifulSoup(text, 'lxml')\n",
    "  wrangled_text = soup.body.get_text()\n",
    "\n",
    "  # Remove all URLs in text\n",
    "  wrangled_text = re.sub(r'https://\\S+|www\\.\\S+', '', wrangled_text)\n",
    "\n",
    "  # Remove all contractions\n",
    "  wrangled_text = contractions.fix(wrangled_text)\n",
    "\n",
    "  return wrangled_text\n",
    "#</SOL>\n",
    "\n",
    "corpus_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Column 0 are reviewas and column 1 are sentiments\n",
    "review = corpus_df.iloc[0, 0]\n",
    "\n",
    "print(colored('\\n============= First review in corpus =============', 'blue'))\n",
    "\n",
    "#<SOL>\n",
    "\n",
    "print(review)\n",
    "\n",
    "#</SOL>\n",
    "\n",
    "print(colored('\\n============= After wrangling result =============', 'blue'))\n",
    "\n",
    "#<SOL>\n",
    "\n",
    "wrangled_review = wrangle_text(review)\n",
    "print(wrangled_review)\n",
    "\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cL978MLjLTey"
   },
   "source": [
    "#### *1.2.2. Tokenization*\n",
    "\n",
    "Tokenization is the process of segmenting a text into words,\n",
    "referred to as **tokens**. This procedure will often also break off punctuation symbols (commas, periods, etc.), phrases, and other possible meaningful elements from the text, such as separate tokens. The list of tokens resulting from tokenization becomes the input for the homogenization stage.\n",
    "\n",
    "The [NLTK Tokenizer Package](https://www.nltk.org/api/nltk.tokenize.html) offers several functions to perform tokenization operations on any text string. Here we will be using the ``wordpunct_tokenize`` function, which allows the separation of punctuation marks. Since sometimes we will be interested in performing the tokenization at the sentence level, we can combine ``wordpunct_tokenize`` and ``sent_tokenize``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFEwe3wSRCbk"
   },
   "source": [
    "##### **Exercise 2**\n",
    "\n",
    "* Tokenize the ``wrangled_review`` at the word level using the ``wordpunct_tokenize`` function. Save the tokenized review in a variable named ``review_tokens``.\n",
    "* Tokenize the the ``wrangled_review`` at the sentence level using the combination of ``wordpunct_tokenize`` and  ``sent_tokenize`` functions. Save the tokenized review in a variable named ``review_tokens_sent``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 894
    },
    "executionInfo": {
     "elapsed": 369,
     "status": "error",
     "timestamp": 1732297796934,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "uuKWO0k8R_g1",
    "outputId": "d3331135-419c-4251-ae3c-c55aa88d49b4"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "print(colored('\\n============= First review in corpus =============', 'blue'))\n",
    "print(wrangled_review)\n",
    "\n",
    "#<SOL>\n",
    "\n",
    "review_tokens = wordpunct_tokenize(wrangled_review)\n",
    "review_tokens_sent = sent_tokenize(wrangled_review)\n",
    "\n",
    "#</SOL>\n",
    "\n",
    "print(colored('\\n============= First review (tokens) =============', 'blue'))\n",
    "print(review_tokens)\n",
    "\n",
    "print(colored('\\n============= First review (tokens sent level) =============', 'blue'))\n",
    "print(review_tokens_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEPiUz14Q3Nq"
   },
   "source": [
    "#### *1.2.3. Homogenization*\n",
    "\n",
    "Homogenization is the process that aims to collapse all semantically equivalent words into a unique representative one. The homogenization process comes from multiple words sharing the same lexeme. For example, ``develop``, ``development``, ``developing``, ``developed``, ``developer``, ``developmental``, and ``developmentally``, are set of words that share the same lexeme or root and, therefore, have a certain relationship of meaning.\n",
    "\n",
    "To homogenize the set of tokens obtained in the previous stage, the following steps need to be performed:\n",
    "\n",
    "\n",
    "1.   **Lower-cased of the tokens**\n",
    "2.   **Elimination of non-alphanumeric characters**, like periods, question marks, and exclamation points.\n",
    "4.   **Word normalization (Stemming/Lemmatization)**, i.e., removing word terminations to preserve the root of the words and ignore grammatical information.\n",
    "\n",
    "Let's see how to apply each of these steps to the previously selected review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTNUuBAYVB_j"
   },
   "source": [
    "##### **Exercise 3**\n",
    "\n",
    "Perform the following two transformations to ``review_tokens``:\n",
    "\n",
    "1.   Convert the tokens to lowercase. Use the ``.lower()`` method.\n",
    "2.   Remove non-alphanumeric tokens. You can detect them using the ``.isalnum()`` method.\n",
    "\n",
    "Save the result in a variable named ``review_tokens_filtered``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZkNRe82SeXS"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "\n",
    "#</SOL>\n",
    "\n",
    "print(review_tokens_filtered[0:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z8eTKMNJezce"
   },
   "source": [
    "At this point, we can choose between applying simple stemming or using lemmatization. We will try both to test their differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XL-2IqBugfyp"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "stemmed_review = [stemmer.stem(el) for el in review_tokens_filtered]\n",
    "print(colored('\\n============= Stemmed review  =============', 'blue'))\n",
    "print(stemmed_review)\n",
    "\n",
    "lemmatized_review = [wnl.lemmatize(el) for el in review_tokens_filtered]\n",
    "print(colored('\\n============= Lemmatized review  =============', 'blue'))\n",
    "print(lemmatized_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLxnHkJXg9NK"
   },
   "source": [
    "One of the advantages of the lemmatizer method is that the result of lemmatization is still a true word, which is more advisable for the presentation of text processing results and lemmatization. Yet, it does not remove grammatical differences (e.g., is\" or \"our\" are preserved and not replaced by the infinitive \"be\")\n",
    "\n",
    "In the following, we will use lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNenHd3WhQ6H"
   },
   "source": [
    "#### *1.2.4. Cleaning*\n",
    "\n",
    "The third step consists of removing those words that are very common in language and do not carry out useful semantic content (articles, pronouns, etc.). For it, we will use the list of stopwords provided by NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gM-Vb069mPKW"
   },
   "source": [
    "##### **Exercise 4**\n",
    "\n",
    "Clean ``lemmatized_review`` by removing all tokens in the stopwords list ``stopwords_en``. Save the result in a variable named ``clean_review``.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05ccOVT5hXOD"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_en = stopwords.words('english')\n",
    "\n",
    "#<SOL>\n",
    "\n",
    "#</SOL>\n",
    "\n",
    "print(colored('\\n============= Lemmatized review  =============', 'blue'))\n",
    "print(lemmatized_review)\n",
    "print(colored('\\n============= Clean lemmatized review  =============', 'blue'))\n",
    "print(clean_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXYrwEkumQ2J"
   },
   "source": [
    "##### **Exercise 5**\n",
    "\n",
    "Complete the function ``prepare_data`` that performs all steps seen above (i.e., from text wrangling to cleaning).\n",
    "\n",
    "Use the ``apply`` function to perform the transformation into all the ``review`` columns of the ``corpus_df`` dataframe and save the result in a new column named ``clean_review``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "48tQOLVAoam4",
    "outputId": "ffea1308-92b2-436c-fe0a-21e3ceb76160"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "def prepare_data(text):\n",
    "# TODO: Implement\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UABnUm07adtq"
   },
   "source": [
    "## **2. Basic Vectorization techniques**\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEJxig0v6o_v"
   },
   "source": [
    "In the following, we are going to be working with Gensim.\n",
    "\n",
    "Gensim is a Python library intended for NLP practitioners. It provides a variety of methods for working with documents in textual format and carrying out semantic analysis tasks such as topic modeling or semantic comparison between documents. For this reason, Gensim is also widely used in Information Retrieval (IR) tasks.\n",
    "\n",
    "Gensim is Open Source and is entirely programmed in Python, so it is easy to modify the code if necessary. The source code is hosted on the [Github development repository](https://github.com/RaRe-Technologies/gensim\n",
    ").\n",
    "\n",
    "Despite being fully developed in Python, Gensim makes extensive use of the Numpy and Scipy libraries that provide highly efficient implementations of certain matrix transformations and mathematical calculations, so Gensim is quite fast. For this reason, Gensim has been adopted by a large number of companies as a core component of complex NLP systems. Gensim is available for use in the main Cloud Computing platforms (AWS, Azure, Google, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkHtR_dE7Y8-"
   },
   "source": [
    "### *2.1. Gensim corpus*\n",
    "\n",
    "When working with Gensim we need to manage collections of documents. In Gensim, a **document** is simply a list of tokens corresponding to a Python string, while a **corpus** is a collection of documents. The simplest way we can work with a corpus is to create a list of documents (i.e., a list of lists of tokens).\n",
    "\n",
    "```\n",
    "# This is a Gensim document\n",
    "doc = ['Any', 'string', 'you', 'want', 'to', 'work', 'with']\n",
    "\n",
    "# This is a Gensim corpus\n",
    "corpus = [doc, 'A second document just to have more than one'.split()]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cM66pQlr71t1"
   },
   "source": [
    "##### **Exercise 6**\n",
    "\n",
    "Generate a corpus to be used in this tutorial. Save it in a variable named ``corpus``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wf1RyXdK70x7"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "\n",
    "#</SOL>\n",
    "\n",
    "print(colored('Number of documents in corpus: '+str(len(corpus)), 'green'))\n",
    "print(colored('\\n============= First review =============', 'blue'))\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iynIQANV9oEx"
   },
   "source": [
    "##### **Exercise 7**\n",
    "\n",
    "Calculate the average number of tokens per review and plot the histogram of the number of tokens per review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71-74RBU95DK"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylvFDqQtSM0h"
   },
   "source": [
    "### *2.2. N-grams detection*\n",
    "\n",
    "Gensim N-gram detection is purely based on the detection of tokens that appear next to each other with high frequency. Gensim `Phraser` can be parameterized to allow some intermediate tokens which are normally considered as links tokens in the English language. However, since we have already carried out lemmatization and stopword removal we can make use of a very simple use of method.\n",
    "\n",
    "Two parameters are necessary:\n",
    "   - `min_count`: Minimum length for N-grams\n",
    "   - `threshold`: Minimum scoring for accepting N-grams. Higher values imply that fewer N-grams are accepted. The threshold is applied on a scoring function that depends on the frequency of the detected N-grams, as well as on the number of isolated occurrences of the component tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctWMZLaTSXlE"
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "\n",
    "phrase_model = Phrases(corpus, min_count=2, threshold=20)\n",
    "\n",
    "print(colored('\\n============= First review in corpus =============', 'blue'))\n",
    "print(corpus[0])\n",
    "corpus = [el for el in phrase_model[corpus]] # We populate corpus again\n",
    "print(colored('\\n============= First review after N-gram replacement =============', 'blue'))\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-lNvsmz9TGP"
   },
   "source": [
    "Let's save our clean reviews in a text file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NU1Yv1Kq9ShB"
   },
   "outputs": [],
   "source": [
    "corpus_df['clean_review'] = corpus\n",
    "\n",
    "with open(\"imdb_lemmas_clean.txt\", 'w', encoding='utf-8') as fout:\n",
    "  for el in corpus_df['clean_review'].values.tolist():\n",
    "    fout.write(' '.join(el) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7AKCo62_WzY"
   },
   "source": [
    "To be able to work with the corpus, we need to vectorize all its documents. To do so, there are two steps we need to carry out:\n",
    "\n",
    "1. Calculate the dictionary\n",
    "2. Transform the documents using the dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yqNJKR0a_1vG"
   },
   "source": [
    "### *2.3. Gensim dictionary*\n",
    "\n",
    "As a first step for vectorizing documents, we need to create a dictionary containing all tokens in our text corpus and assign an integer identifier to each one of them.\n",
    "\n",
    "The following code fragment generates such a dictionary and shows the first tokens in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cubEHlmKAQgD"
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create dictionary of tokens\n",
    "D = Dictionary(corpus)\n",
    "n_tokens = len(D)\n",
    "\n",
    "print('The positive dictionary contains', n_tokens, 'terms')\n",
    "print('First terms in the dictionary:')\n",
    "for n in range(10):\n",
    "    print(str(n), ':', D[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GjXFSoBaBz36"
   },
   "source": [
    "\n",
    "Saved\n",
    "207 words\n",
    "The dictionary object contains several attributes and methods that can be useful for carrying out some cleaning tasks. You can check the available methods using\n",
    "\n",
    "```\n",
    "dir(D)\n",
    "```\n",
    "\n",
    "Some of the most useful methods that we will use are:\n",
    "\n",
    "   - ```add_documents```: updates the dictionary processing new documents\n",
    "\n",
    "   - ```merge_with```: merges two dictionaries\n",
    "\n",
    "   - ```save```, ```save_as_text```, ```load```, ```load_from_text```: can be used to give persistence to the dictionary and reading a previously calculated dictionary\n",
    "\n",
    "   - ```id2token```: This is a Python dictionary for the mapping tokenid (a number) -> token (text representation). You can check that ```D[n]``` is equivalent to ```D.id2token[n]```\n",
    "\n",
    "   - ```token2id```: A Python dictionary for the reverse mapping token -> tokenid\n",
    "\n",
    "   - ```items```, ```keys```, ```values```, ```iteritems```, ```iterkeys```, ```itervalues```: Can be used to obtain al items (tokenid, token), all tokenids, or all token texts, or to iterate over them.\n",
    "\n",
    "   - ```dfs```: A Python dictionary for the mapping tokenid -> Number of documents where the token appears\n",
    "\n",
    "   - ```filter_tokens```, ```filter_extremes```, ```filter_n_most_frequent```: are used to remove elements from the dictionary, and ```compactify```is used to reassign tokenids to tokens for a more efficient representation.\n",
    "\n",
    "   - ```doc2bow```: converts a document into its Bag of Words Representation\n",
    "\n",
    "   - ```doc2idx```: transforms a document into a sequence of the tokenids of the words of the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1E2Vq7ZCt21"
   },
   "source": [
    "##### **Exercise 8**\n",
    "\n",
    "1. Obtain a dataframe with 2 columns: `token` and `ndocs`, corresponding to the text of each token and the number of documents where the token appears\n",
    "\n",
    "2. Sort the dataframe according to column `ndocs`.\n",
    "\n",
    "3. How many tokens appear in exactly one document? Remove them from the dataframe.\n",
    "\n",
    "4. What are the most and less common tokens in the dictionary in terms of document occurrence?\n",
    "\n",
    "3. Plot a histogram of the number of token appearances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlmfLWJsDDik"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUU9rKrkDTdL"
   },
   "source": [
    "Next, we will filter out terms that appear in too few or too many of the documents in the dataset. This makes sense because:\n",
    "\n",
    "   - terms that appear in most documents are probably not very informative in the general context of a particular corpus\n",
    "   - terms that appear in a very reduced number of documents are not useful to find repetitive patterns across documents. In fact, in many cases, we find that many of the words that are eliminated for this reason can be typos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_UcE8geDSuT"
   },
   "outputs": [],
   "source": [
    "no_below = 4 #Minimum number of documents to keep a term in the dictionary\n",
    "no_above = .80 #Maximum proportion of documents in which a term can appear to be kept in the dictionary\n",
    "\n",
    "D.filter_extremes(no_below=no_below,no_above=no_above)\n",
    "n_tokens = len(D)\n",
    "\n",
    "print('The dictionary contains', n_tokens, 'terms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_hNAvi8DaNv"
   },
   "source": [
    "You can check dictionary size has been considerably reduced with respect to the original vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpdMDkV6Qrox"
   },
   "outputs": [],
   "source": [
    "corpus_def = []\n",
    "for sent in corpus:\n",
    "  aux = [token for token in sent if token in D.token2id.keys()]\n",
    "  corpus_def.append(aux)\n",
    "\n",
    "corpus_df['clean_review'] = corpus_def"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rISd3VD_-Yi"
   },
   "source": [
    "### *2.3. Bag-Of-Words (BoW)*\n",
    "\n",
    "Next, let us create a numerical version of our corpus using the `doc2bow` method. In general, `D.doc2bow(token_list)` transforms any list of tokens into a list of tuples `(token_id, n)`, one per each token in `token_list`, where `token_id` is the token identifier (according to dictionary `D`) and `n` is the number of occurrences of such a token in `token_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FJdsQvFbDpht"
   },
   "outputs": [],
   "source": [
    "reviews_bow = [D.doc2bow(doc) for doc in corpus]\n",
    "\n",
    "n_review = 1000\n",
    "print(colored('============= Review (lemmas) =============', 'blue'))\n",
    "print(' '.join(corpus[n_review]))\n",
    "\n",
    "print(colored('\\n============= Sparse vector representation =============', 'blue'))\n",
    "print(reviews_bow[n_review])\n",
    "\n",
    "print(colored('\\n============= Word counts for the review =============', 'blue'))\n",
    "print(list(map(lambda x: (D[x[0]], x[1]), reviews_bow[n_review])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQZkdCPiEL1A"
   },
   "source": [
    "Note that we can interpret each element of corpus_bow as a `sparse_vector`. For example, a list of tuples\n",
    "\n",
    "    [(0, 1), (3, 3), (5,2)]\n",
    "\n",
    "for a dictionary of 10 elements can be represented as a vector, where any tuple `(id, n)` states that position `id` must take value `n`. The rest of the positions must be zero.\n",
    "\n",
    "    [1, 0, 0, 3, 0, 2, 0, 0, 0, 0]\n",
    "\n",
    "As a summary, we have obtained the following variables that will be relevant for the next sections:\n",
    "\n",
    "   * `D`: A Gensim dictionary. Term strings can be accessed using numeric identifiers. For instance, `D[0]` contains the string corresponding to the first position in the BoW representation.\n",
    "   * `mycorpus_bow`: BoW corpus. A list containing an entry per project in the dataset, and consisting of the (sparse) BoW representation for the abstract of that project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_RZWSiM_-kD"
   },
   "source": [
    "### *2.4. TF-IDF vectorization*\n",
    "\n",
    "Gensim TFIDF representation of a document is computed as\n",
    "\n",
    "$$x_{ij} = \\text{freq}_{ij} \\log_2 \\frac{\\# docs}{\\# docs_j}$$\n",
    "\n",
    "where:\n",
    "\n",
    "   - $x_{ij}$ is the component of the TFIDF representation of document $i$ corresponding to term $j$\n",
    "   - $\\text{freq}_{ij}$ is the frequency of term $j$ in a document $i$ (i.e., number of occurrences divided by the number of tokens)\n",
    "   - $\\# docs$ is the total number of documents in the corpus\n",
    "   - ${\\# docs_j}$ is the number of documents in the corpus containing term $j$\n",
    "\n",
    "In this way, terms that appear in fewer documents get emphasized over common terms appearing in many documents.\n",
    "\n",
    "Gensim offers the possibility to change the *term frequency* and *inverse document frequency* calculation terms, but we will keep the defaults.\n",
    "\n",
    "Note that, contrary to the Bag of Words (BoW) representation, the TFIDF representation does not depend just on the tokens of each document, but gets affected by the whole corpus through the IDF factor.\n",
    "\n",
    "Gensim considers TFIDF as a model on its own and deals with it similarly to what is done with other models. Creating a TFIDF model is very simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "5FXpTZ1REXqB",
    "outputId": "5a0cd6f5-17df-421f-8f54-cbdaa2108b66"
   },
   "outputs": [],
   "source": [
    "from gensim.models import TfidfModel\n",
    "\n",
    "tfidf = TfidfModel(reviews_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iN_o_jSWEeEc"
   },
   "source": [
    "A **TFIDF model cannot be updated** adding more documents. Otherwise, we would lose consistency, i.e., the TFIDF representation for a particular document would change before and after the TFIDF model gets updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffXsDP8xEhZd"
   },
   "source": [
    "From now on, `tfidf` can be used to convert any vector from the old representation (bow integer counts) to the new one (TFIDF real-valued weights), or to apply a transformation to a whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G_Hwm8PfEkU8"
   },
   "outputs": [],
   "source": [
    "reviews_tfidf = tfidf[reviews_bow]\n",
    "n_project = 1000\n",
    "print(colored('============= TFIDF representation for the project =============', 'blue'))\n",
    "print(reviews_tfidf[n_review])\n",
    "\n",
    "print(colored('\\n============= TFIDF applying the transformation only to the document =============', 'blue'))\n",
    "print(tfidf[reviews_bow[n_review]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-WX-jNJpEwLS"
   },
   "source": [
    "As for BOW, TFIDF provides a sparse document representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H87nLNNLE8b6"
   },
   "source": [
    "### *2.5. Memory efficient computation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDw_YwGsGXxV"
   },
   "source": [
    "In the previous examples, the construction of the dictionary and the transformation of the corpus to BoW or TFIDF format required that said corpus of documents be available in a list in the execution environment, and therefore required it to be stored in RAM. For a small corpus, this is not a problem, but it can be an important limitation when dealing with a large corpus with millions or tens of millions of documents. These corpora are becoming more and more common in certain applications (consider Wikipedia entries, user opinions on large e-commerce platforms, processing of medical records, etc.).\n",
    "\n",
    "One of the advantages of Gensim is that its implementation makes it easy to work with a corpus of these sizes. As explained in the Gensim documentation:\n",
    "\n",
    "> Note that the corpus above resides fully in memory, as a plain Python list. In this simple example, it doesn’t matter much, but just to make things clear, let’s assume there are millions of documents in the corpus. Storing all of them in RAM won’t do. Instead, let’s assume the documents are stored in a file on disk, one document per line. Gensim only requires that a corpus must be able to return one document vector at a time.\n",
    "\n",
    ">The full power of Gensim comes from the fact that a corpus doesn’t have to be a list, a NumPy array, a Pandas dataframe, or whatever. Gensim accepts any object that, when iterated over, successively yields documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhqeH_cTGaUO"
   },
   "source": [
    "The next fragment of code illustrates how the dictionary can be created from a corpus stored in a text file. You just need to create an iterator that returns a document at each iteration and keeps adding documents to the dictionary. Note that during the execution of the code, only one document is kept in memory at every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpbGsYThFPs1"
   },
   "outputs": [],
   "source": [
    "class IterableCorpus_fromfile:\n",
    "    def __init__(self, filename):\n",
    "        self.__filename = filename\n",
    "    def __iter__(self):\n",
    "        for line in open(self.__filename):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield line.lower()\n",
    "\n",
    "MyIterCorpus = IterableCorpus_fromfile('imdb_lemmas_clean.txt')\n",
    "newD = Dictionary()\n",
    "for doc in MyIterCorpus:\n",
    "  newD.add_documents([doc.split()])\n",
    "no_below = 4 # Minimum number of documents to keep a term in the dictionary\n",
    "no_above = .80 # Maximum proportion of documents in which a term can appear to be kept in the dictionary\n",
    "newD.filter_extremes(no_below=no_below,no_above=no_above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yKIzbxUXFU4P"
   },
   "source": [
    "The code above can be further simplified if the iterator already carries out the tokenization of each document. In that case, the dictionary can be created with a simple command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPFQok6ME8Am"
   },
   "outputs": [],
   "source": [
    "class IterableCorpus_fromfile:\n",
    "    def __init__(self, filename):\n",
    "        self.__filename = filename\n",
    "    def __iter__(self):\n",
    "        for line in open(self.__filename):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield line.lower().split()\n",
    "\n",
    "MyIterCorpus = IterableCorpus_fromfile('imdb_lemmas_clean.txt')\n",
    "newD = Dictionary(MyIterCorpus)\n",
    "no_below = 4 # Minimum number of documents to keep a term in the dictionary\n",
    "no_above = .80 # Maximum proportion of documents in which a term can appear to be kept in the dictionary\n",
    "newD.filter_extremes(no_below=no_below,no_above=no_above)\n",
    "\n",
    "print('Number of documents processed:', newD.num_docs)\n",
    "print('Number of elements in dictionary:', len(newD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jI5R6rRtGIEA"
   },
   "source": [
    "### *2.7. Compatibility with Numpy and Scipy*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aL6Ztp3ZGT99"
   },
   "source": [
    "Gensim contains efficient functions to convert Gensim Corpus (BoW, TFIDF) to Numpy dense matrices or Scipy Sparse Matrices. This can be useful, e.g., if we wish to use the vectorial representation of a Gensim corpus to train a classification or regression model using sklearn.\n",
    "\n",
    "Similarly, we also have functions to convert Numpy or Scipy matrices into Gensim representation.\n",
    "\n",
    "More information on the available utilities can be found in the [Gensim API matutils documentation](https://radimrehurek.com/gensim/matutils.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgLdeQAhG4Y9"
   },
   "source": [
    "Sklearn also includes functions for tokenization and vectorization of documents. Specifically, it has the functions:\n",
    "* [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) which implements both tokenization and word count (BoW) in a single class.\n",
    "* [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html#sklearn.feature_extraction.text.TfidfTransformer) which is responsible for obtaining the TF-IDF representation from a BoW representation.\n",
    "\n",
    "* [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) which is equivalent to using `CountVectorizer()` followed by `TfidfTransformer()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VoWITECrGhus"
   },
   "source": [
    "## **3. Sentiment Analysis with BoW and TF-IDF representations**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnFHjJ6qbZuc"
   },
   "source": [
    "Let's start by loading the problems labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "wcbywHiDbdu-",
    "outputId": "3c681a18-c386-4e93-c29c-725921e6ba24"
   },
   "outputs": [],
   "source": [
    "def get_binary_label(sentiment):\n",
    "  return 1 if sentiment == \"positive\" else 0\n",
    "\n",
    "corpus_df['binary_sentiment'] = corpus_df['sentiment'].apply(get_binary_label)\n",
    "\n",
    "Y = corpus_df['binary_sentiment'].values\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNCsJPzsj9mc"
   },
   "source": [
    "And save all the changes we have made in ``corpus_df`` for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rWYfb-r_JXH"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def pickler(file: str, ob):\n",
    "    \"\"\"Pickle object to file\"\"\"\n",
    "    with open(file, 'wb') as f:\n",
    "        pickle.dump(ob, f)\n",
    "    return 0\n",
    "\n",
    "pickler(\"corpus_df_imbdb.pickle\",corpus_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXBeXFgYcTeE"
   },
   "source": [
    "Since we have carried out the vectorization with Gensim,  we have to convert our vector representation into NumPy arrays so we can use Sklearn's classifiers. To do this, Gensim includes two functions: [corpus2dense](https://tedboy.github.io/nlps/generated/generated/gensim.matutils.corpus2dense.html), [corpus2csc](https://tedboy.github.io/nlps/generated/generated/gensim.matutils.corpus2csc.html). In general, when dealing with huge corpora, we will be interested in managing the sparse form of the data to save on computational costs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vGrhOlDIc7oc"
   },
   "outputs": [],
   "source": [
    "from gensim.matutils import corpus2dense, corpus2csc\n",
    "\n",
    "n_tokens = len(D)\n",
    "num_docs = len(reviews_bow)\n",
    "\n",
    "# Convert BoW representacion\n",
    "corpus_bow_dense = corpus2dense(reviews_bow, num_terms=n_tokens, num_docs=num_docs).T\n",
    "corpus_bow_sparse = corpus2csc(reviews_bow, num_terms=n_tokens, num_docs=num_docs).T\n",
    "\n",
    "# Convert TFIDF representacion\n",
    "corpus_tfidf_dense = corpus2dense(reviews_tfidf, num_terms=n_tokens, num_docs=num_docs).T\n",
    "corpus_tfidf_sparse = corpus2csc(reviews_tfidf, num_terms=n_tokens, num_docs=num_docs).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDkvoVRsmdak"
   },
   "source": [
    "##### **Exercise 9**\n",
    "\n",
    "Train an SVM classifier with the BoW representation of the IMDB dataset. Use the Sklearn function ``train_test_split`` to split the BOW representation of the reviews with a $70/30$ ratio each and a random state of $42$. Find the best hyperparameters for the SVM (``C`` and ``kernel``) via cross-validation with [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV). Evaluate the performance of the classifier based on the [R2 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FxRQWMoq-bSq"
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RN0zvaZMqZL"
   },
   "source": [
    "##### **Exercise 10**\n",
    "\n",
    "Mimic the steps from Exercise 9 to train an SVM classifier with the TF-IDF representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwZn_P8rdLOk"
   },
   "outputs": [],
   "source": [
    "# <SOL>\n",
    "\n",
    "# </SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LriaFb8wbEyP"
   },
   "source": [
    "---\n",
    "In this first laboratory, we have covered the necessary preprocessing steps that need to be applied to a text corpus previous to its vectorization using several state-of-the-art Python libraries. We have then seen how to obtain BoW and TFIDF representations based on the Gensim library and how to use them for a Sentiment Analysis problem.\n",
    "\n",
    "While we will see that Bag-of-Words and TF-IDF as they neither capture the context of words nor allow for similarity comparison, it is still important to know how they work and how to use them, since they still provide quite good results in some tasks, as we have seen in this notebook.\n",
    "\n",
    "To finish, it is also important that you get confident with the Genism library as you will be using it a lot in this course!\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1oNpnQH5djzIh8tPM_SMCO2W_6vgtygje",
     "timestamp": 1668155406859
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
