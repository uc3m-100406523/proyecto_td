{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eU_gqVHcSuHE"
   },
   "source": [
    "# <font color='#2B4865'>**Hugging Face ðŸ¤— Transformers Tutorial III**</font>\n",
    "\n",
    "---\n",
    "### Natural Language Processing\n",
    "Date: Jan 11, 2023\n",
    "\n",
    "Last Updated: Nov 30, 2023\n",
    "\n",
    "Author: Lorena Calvo-BartolomÃ© (lcalvo@pa.uc3m.es)\n",
    "\n",
    "Version 1.0\n",
    "\n",
    "---\n",
    "This notebook is based on the [Hugging Face course](https://huggingface.co/course/chapter1/1) and documentation available at the Hugging Face website.\n",
    "\n",
    "It constitutes the third and last tutorial notebook on the usage of Hugging Face libraries as well as its application for solving a series of NLP tasks. In particular, we will be covering how to carry out Text Generation.\n",
    "\n",
    "---\n",
    "\n",
    "<font color='#E0144C'>**For this notebook's execution, we highly encourage you to use Google Colaboratory. While for the inference part it is not necessary, you will highly speed up the execution if you make use of a GPU. For doing so, follow the following steps:**</font>\n",
    "\n",
    "<font color='#E0144C'>**1. Connect to hosted runtime**</font>\n",
    "\n",
    "<font color='#E0144C'>**2. Enable GPU setting by clicking Edit -> Notebook Settings -> Select GPU in Hardware Acceleration Tab -> Save**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_ak68EcROir"
   },
   "source": [
    "## <font color='#2B4865'>Installing necessary packages, imports and auxiliary functions</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNmR0A9v3gG7"
   },
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "import importlib, os\n",
    "\n",
    "necessary_packages = ['transformers[sentencepiece]', 'datasets', 'colored', 'evaluate', 'accelerate']\n",
    "def import_missing(packages):\n",
    "  for p in packages:\n",
    "    try:\n",
    "      mod = importlib.import_module(p)\n",
    "      print(f\"Package {p} already installed!\")\n",
    "      packages.remove(p)\n",
    "    except ModuleNotFoundError:\n",
    "      print(f\"Installing package {p}\")\n",
    "      with open(\"requirements.txt\", 'w') as f:\n",
    "        f.write(\"\\n\".join(str(i) for i in packages))\n",
    "  if os.path.isfile(\"requirements.txt\"):\n",
    "    %pip install --quiet -r \"requirements.txt\"\n",
    "\n",
    "import_missing(necessary_packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EB842ZODAnIz"
   },
   "outputs": [],
   "source": [
    "%pip install -q typing-extensions==4.5.0 kaleido ftfy bitsandbytes==0.35.0 gradio natsort safetensors xformers cohere openai tiktoken pydantic-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VszD2tguTJcS"
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "import scipy\n",
    "from colored import Fore, Back, Style\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# Figures plotted inside the notebook\n",
    "%matplotlib inline\n",
    "# High quality figures\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Figues size\n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(action='ignore',module='gradio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lhwq7n9ETRuL"
   },
   "outputs": [],
   "source": [
    "# To wrap long text lines\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)\n",
    "\n",
    "# For fancy table Display\n",
    "%load_ext google.colab.data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTPdyX5-bb-w",
    "outputId": "9ac09bde-91e8-41ea-c135-e00ead830c14",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Google Drive integration\n",
    "\n",
    "# Libraries to work with Google Drive and the file system\n",
    "from google.colab import drive\n",
    "import os, sys\n",
    "\n",
    "# Drive is mounted\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Current directory is shown\n",
    "print(os.getcwd())\n",
    "\n",
    "# We change to work directory\n",
    "directory_path = \"/content/drive/MyDrive/Colab Notebooks/proyecto_td/reference_code/Hugging_Face_Transformers_Tutorial_III_students/\"  # Define directory_path here\n",
    "if not os.path.exists(directory_path):\n",
    "  os.makedirs(directory_path)\n",
    "  print(f\"Directory created: {directory_path}\")\n",
    "os.chdir(directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZqNyqb80hXm"
   },
   "source": [
    "## <font color='#2B4865'>**4. Text Generation**\n",
    "---\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5Z5JWlx1OJ6"
   },
   "source": [
    "Text Generation is the task of producing new text. These models can, for instance, fill in incomplete text or paraphrase.\n",
    "\n",
    "Some use cases include:\n",
    "\n",
    "* **Story Generation**: By receiving an input like \"*Once upon a time*\", a story generation model can create a story based on those words as we saw with GPT-2.\n",
    "* **Code Generation**: We can train a text generation model on code from scratch to help with repetitive coding tasks.\n",
    "\n",
    "* **Text-to-Text Generation Models**, which are trained to learn the mapping between a pair of texts (e.g. translation from one language to another). They are trained with multi-tasking capabilities, they can accomplish a wide range of tasks, including summarization, translation, and text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inF9tmtNz27j"
   },
   "source": [
    "##### <font color='#2B4865'>**Demo**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OahMLShphX3x"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytgpj_3_z0KK"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed, pipeline\n",
    "\n",
    "#https://huggingface.co/spaces/lvwerra/codeparrot-generation\n",
    "\n",
    "title = \"CodeParrot Generator ðŸ¦œ\"\n",
    "description = \"This is a subspace to make code generation with [CodeParrot](https://huggingface.co/lvwerra/codeparrot), it is used in a larger [space](https://huggingface.co/spaces/loubnabnl/Code-generation-models-v1) for model comparison. For more flexibilty in sampling, you can find another demo for CodeParrot [here](https://huggingface.co/spaces/lvwerra/codeparrot-generation).\"\n",
    "example = [\n",
    "    [\"def print_hello_world():\", 8, 0.6, 42],\n",
    "    [\"def get_file_size(filepath):\", 40, 0.6, 42],\n",
    "    [\"def count_lines(filename):\", 40, 0.6, 42],\n",
    "    [\"def count_words(filename):\", 40, 0.6, 42]]\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codeparrot/codeparrot-small\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"codeparrot/codeparrot-small\", low_cpu_mem_usage=True)\n",
    "\n",
    "\n",
    "def code_generation(gen_prompt, max_tokens, temperature=0.6, seed=42):\n",
    "    set_seed(seed)\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    generated_text = pipe(gen_prompt, do_sample=True, top_p=0.95, temperature=temperature, max_new_tokens=max_tokens)[0]['generated_text']\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=code_generation,\n",
    "    inputs=[\n",
    "        gr.Textbox(lines=10, label=\"Input code\"),\n",
    "        gr.inputs.Slider(\n",
    "            minimum=8,\n",
    "            maximum=256,\n",
    "            step=1,\n",
    "            default=8,\n",
    "            label=\"Number of tokens to generate\",\n",
    "        ),\n",
    "        gr.inputs.Slider(\n",
    "            minimum=0,\n",
    "            maximum=2,\n",
    "            step=0.1,\n",
    "            default=0.6,\n",
    "            label=\"Temperature\",\n",
    "        ),\n",
    "        gr.inputs.Slider(\n",
    "            minimum=0,\n",
    "            maximum=1000,\n",
    "            step=1,\n",
    "            default=42,\n",
    "            label=\"Random seed to use for the generation\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Predicted code\", lines=10),\n",
    "    examples=example,\n",
    "    layout=\"horizontal\",\n",
    "    theme=\"peach\",\n",
    "    description=description,\n",
    "    title=title\n",
    ")\n",
    "iface.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2cy1_0FZcVn"
   },
   "source": [
    "##### <font color='#2B4865'>**Architecture for approaching the task**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3Qpf7Mcxxqx"
   },
   "source": [
    "The most popular models for this task are GPT-based models (such as GPT-2). Since these models are trained on unlabeled data, we just need plain text to train our own model to generate a wide variety of documents, from code to stories. Regarding the Text-to-Text Generation Models, its most popular variants are T5, T0 and BART."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5sFf1B0zj76"
   },
   "source": [
    "##### <font color='#2B4865'>**Evaluation metrics**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvNSvD15Ze4X"
   },
   "source": [
    "**Typical metrics** for Text Generation are:\n",
    "\n",
    "* **Cross Entropy**, which is a metric that calculates the difference between two probability distributions. Each probability distribution is the distribution of predicted words.\n",
    "* **Perplexity**, which is the exponential of the cross-entropy loss. It evaluates the probabilities assigned to the next word by the model. Lower perplexity indicates better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IRE_lhVVWkV"
   },
   "source": [
    "###Â <font color='#2B4865'>*4.1. Inference*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4epAcBWUPHke"
   },
   "source": [
    "Performing inference for a text generation task is pretty similar to what we saw in former tutorials for the studied tasks. For basic text generation tasks (i.e., stories or code generation), we need to provide the model to use in conjunction with the task identifier ``\"text-generation\"``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wI3N6CC6SkyP"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "checkpoint_name = \"gpt2\"\n",
    "text_generator = pipeline(\"text-generation\",\n",
    "                          model=checkpoint_name,\n",
    "                          pad_token_id=tokenizer.eos_token_id)                  # Prevents warning during decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "usKcRKwdnjPh"
   },
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time\"\n",
    "print(text_generator(prompt, max_length=50, do_sample=True, top_p=0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbOIhnSHS2cJ"
   },
   "source": [
    "The model is generating a random text with a total maximal length of $50$ tokens from context â€œ*Once upon a time*â€. To create text, the pipeline object invokes the method ``PreTrainedModel.generate()``. The default arguments for this method can be overridden in the pipeline.\n",
    "\n",
    "Below is an example of text generation using directly the GPT2 model and its tokenizer, which includes calling ``generate()`` directly. Note that for the instantiation of the model alone, we utilize a ``CausalLM`` head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JDUxrJ3QzSZ"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(checkpoint_name)\n",
    "gpt2 = AutoModelForCausalLM.from_pretrained(checkpoint_name)\n",
    "gpt2.config.pad_token_id = gpt2.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OuKVICt8G9Vm"
   },
   "outputs": [],
   "source": [
    "tokenized_prompt = gpt2_tokenizer(prompt, return_tensors=\"pt\")\n",
    "output = gpt2.generate(**tokenized_prompt, max_length=50, do_sample=True, top_p=0.9)\n",
    "print(f\"{gpt2_tokenizer.batch_decode(output)[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srFTVMJkl5UM"
   },
   "source": [
    "The following code snippet creates a **pipeline for Code generation**, based on CodeParrot, similar to what is done above for the Gradio app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7qnHvlThNgTC"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"codeparrot/codeparrot-small\")\n",
    "\n",
    "codeparrot = pipeline(\"text-generation\", model=\"codeparrot/codeparrot-small\")\n",
    "outputs = codeparrot(\"def hello_world():\", max_length=50, pad_token_id=tokenizer.eos_token_id)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qvZL9jONbcY"
   },
   "source": [
    "**Text-to-Text generation models** have a separate pipeline called ``text2text-generation``. This pipeline takes an input containing the sentence including the task and returns the output of the accomplished task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MuMJu6hN412"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text2text_generator = pipeline(\"text2text-generation\", model=\"t5-base\")\n",
    "\n",
    "# Question Answering\n",
    "question = \"What is 42 ?\"\n",
    "context = \"42 is the answer to life, the universe and everything\"\n",
    "output = text2text_generator(\"question: \" + question + \" \" + context)\n",
    "print(f\"{question} --> {output[0]['generated_text']}\")\n",
    "\n",
    "# Translation\n",
    "source_sent = \"I'm very happy\"\n",
    "output = text2text_generator(\"translate from English to French: \" + source_sent)\n",
    "print(f\"{source_sent} --> {output[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRGp6Ll4V5yR"
   },
   "source": [
    "###Â <font color='#2B4865'>*4.2. Decoding methods for language generation*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Y8wvJwqTZpj"
   },
   "source": [
    "There exist different decoding strategies that are used by **auto-regressive language generation models** (i.e., decoder models), being the currently most prominent one the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdgQJj9b4MC1"
   },
   "source": [
    "1. **Greedy search**: At each time step, it selects **the word with the highest probability as its next word**, which is given by:\n",
    "\n",
    "  $$w_t = argmax_{w}P(w | w_{1:t-1})$$\n",
    "\n",
    "    <br><center><img src=\"https://drive.google.com/uc?id=1TPmWTY4bljqyRAB0qTXJVtf3qQ5CRBbT\" width=\"40%\"></center><br>\n",
    "\n",
    "  Starting from the word \"*The*\", the algorithm greedily chooses the next word of highest probability \"*nice*\" and so on, so that the final generated word sequence is *{\"The\", \"nice\", \"woman\"}*, with a probability of $0.5 \\times 0.4 = 0.2$.\n",
    "\n",
    "  The **way of generating text following a greedy search approach** is as follows:\n",
    "\n",
    "  ```\n",
    "  greedy_output = model.generate(input_ids, do_sample=False)\n",
    "  ```\n",
    "\n",
    "  The main drawbacks of this approach are:\n",
    "  * The model starts repeating itself quickly\n",
    "  * It misses high-probability words hidden behind a low-probability word, e.g., the word \"*has*\" with its high conditional probability of $0.9$ is hidden behind the word \"*dog*\", which has only the second-highest conditional probability, so that greedy search misses the word sequence *{\"The\", \"dog\", \"has\"}*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b51fUKCqqA3f"
   },
   "source": [
    "2. **Beam search**: It reduces the risk of missing hidden high-probability word sequences by keeping the most likely ``num_beams`` of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability. For example, with ``num_beams=2``:\n",
    "\n",
    "    <br><center><img src=\"https://drive.google.com/uc?id=1Hi0dDz4PKe6dOHOZLbmPmP-1g6_7J1Ia\" width=\"40%\"></center><br>\n",
    "\n",
    "  At time step 1, besides the most likely hypothesis *{\"The\", \"nice\"}*, beam search also keeps track of the second most likely one *{\"The\", \"dog\"}*. At time step 2, it finds that the word sequence *{\"The\", \"dog, \"has\"}*, has with $0.36$ a higher probability than *{\"The\", \"nice\", \"woman\"}*, which has $0.2$, thus finding the most likely word sequence.\n",
    "\n",
    "  The way of generating text following a beam search approach is by setting  ``num_beams > 1`` and ``early_stopping=True`` so that generation is finished when all beam hypotheses reached the EOS token. We can further improve the result to reduce repetitions of the same word sequences by **introducing n-grams**. The most common n-grams penalty makes sure that no n-gram appears twice by manually setting the probability of the next words that could create an already seen n-gram to 0, e.g.:\n",
    "  ```\n",
    "  beam_output = model.generate(input_ids, num_beams=5, no_repeat_ngram_size=2, early_stopping=True, do_sample=False)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cP6UWjOvrU1e"
   },
   "source": [
    "3. **Sampling**: It refers to **randomly picking the next word $w_t$ according to its conditional probability** distribution $w_t \\sim P(w|w_{1:t-1})$:\n",
    "\n",
    "    <br><center><img src=\"https://drive.google.com/uc?id=14rILPEf-c43F8vXc_o2hFcRNoWQlCvod\" width=\"40%\"></center><br>\n",
    "\n",
    "  The word \"*car*\" is sampled from $P(w | \\text{\"The\"})$, followed by sampling \"*drives*\" from $P(w | \\text{\"The\"}, \\text{\"car\"})$.\n",
    "\n",
    "  We can activate sampling by setting ``do_sample=True``.\n",
    "\n",
    "  One issue with sampling is that the models tend to produce incomprehensible nonsense. This may be solved by sharpening the conditional probability distribution by raising the likelihood of high-probability words and decreasing the likelihood of low-probability words, i.e.,  by lowering the softmax's so-called ``temperature``:\n",
    "  \n",
    "      <br><center><img src=\"https://drive.google.com/uc?id=1nz12FfdJIcf92LhS3pg6K5-PeUMQ_jrI\" width=\"40%\"></center><br>\n",
    "\n",
    "  The conditional next word distribution of step $t=1$ becomes much sharper leaving almost no chance for the word \"*car*\" to be selected.\n",
    "\n",
    "  In practice, the Transformers library provides two different sampling methods:\n",
    "\n",
    "  *   **Top-k sampling:** The $K$ most likely next words are filtered and the probability mass is redistributed among only those $K$ next words. GPT2 adopted this sampling scheme, which was one of the reasons for its success in story generation:\n",
    "  \n",
    "    <br><center><img src=\"https://drive.google.com/uc?id=1PrJR5ZCe8-T4-1lsPt1tV6NCq7bSen95\" width=\"70%\"></center><br>\n",
    "\n",
    "    Having set $K = 6$, in both sampling steps we limit our sampling pool to 6 words.\n",
    "\n",
    "    We activate top-K by setting ``top_k`` to the size of the candidate set:\n",
    "\n",
    "    ```\n",
    "    sample_output = model.generate(input_ids, do_sample=True, top_k=50)\n",
    "    ```\n",
    "\n",
    "  *   **Top-p (nucleus) sampling:** Instead of sampling only from the most likely K words, Top-p sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. The probability mass is then redistributed among this set of words. This way, the size of the set of words (a.k.a the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution:\n",
    "\n",
    "    <br><center><img src=\"https://drive.google.com/uc?id=11bTzQPA_cNIPfJwaTMcwPMHfXF6aRYUn\" width=\"70%\"></center><br>\n",
    "\n",
    "    Having set $p=0.92$, it picks the minimum number of words to exceed together $p=92\\%$ of the probability mass ($V_{\\text{top-p}}$). In the first example, this included the 9 most likely words, whereas it only has to pick the top 3 words in the second example to exceed 92%.\n",
    "  \n",
    "    We activate Top-p sampling by setting ``top_p``to 0 < top_p < 1 and ``top_k`` to 0:\n",
    "\n",
    "    ```\n",
    "    sample_output = model.generate(input_ids, do_sample=True, top_p=0.92, top_k=0)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drzRuV0Yn5HB"
   },
   "source": [
    "###### **Exercise 4.1**\n",
    "\n",
    "Take the prompt we defined in 4.1 (``prompt = \"Once upon a time\"``) and generate text with the GPT-2 model according to the different decoding methods explained above. When possible, carry out the fine-tuning of parameters (e.g., ``num_beams`` for greedy search, ``temperature`` for sampling methods, etc.).\n",
    "\n",
    "Compare the results obtained from the different methods and comment below on the differences you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyizMfXHnIpl"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "# TODO: Define tokenizer, model and tokenize the promt we are going to use\n",
    "\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVmM5wBAjDsT"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "# Greedy search\n",
    "\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7vMpGwUjvsv"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "# Beam search\n",
    "\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eLpYPstrjwML"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "# Top-k sampling\n",
    "\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KJ2KJHPvjwa8"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "# Top-p sampling\n",
    "\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzEnVLb74Fna"
   },
   "source": [
    "###Â <font color='#2B4865'>*4.2. Fine-tuning*</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1sjs11laozq"
   },
   "source": [
    "Fine-tuning a model for text generation is not different from what we have seen for previous tasks, just with a couple of peculiarities regarding preprocessing. Let's see the latter by using the [WikiText](https://huggingface.co/datasets/wikitext) language modeling dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvNOWmPxdXyh"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "datasets = load_dataset('wikitext', 'wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D2gihnSXdhu8"
   },
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cOcnGrLSdntP"
   },
   "source": [
    "As checkpoint for this example, we will use the ``distilgpt2`` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kG2VOZsSdxMS"
   },
   "outputs": [],
   "source": [
    "checkpoint_name = \"distilgpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5c5_ZK-zdZG2"
   },
   "source": [
    "A **standard preprocessing step** for both **auto-regressive and masked language modeling** is to **concatenate all the instances** and then **split the whole corpus into chunks of similar size**. This is quite different from our usual approach, where we simply tokenize individual examples. The reason for this is that if individual samples are too long, they may be truncated, resulting in the loss of information that may be valuable for our task.\n",
    "\n",
    "The approach is then to first tokenize our corpus as usual, but without setting the ``truncation=True`` option in our tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSThWXEReLx5"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_name)\n",
    "\n",
    "print(tokenizer.is_fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qzy9bxm9eVbE"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1tiRyvIef_Y"
   },
   "source": [
    "Having our text tokenized, the next step is to concatenate all our texts together and then split the result into small chunks of certain block size. The desired block size would be the maximum length with which our model was pretrained; yet, this might be too big to fit in the GPU RAM, so let's just work with $128$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3WV68cTgA6m"
   },
   "outputs": [],
   "source": [
    "print(tokenizer.model_max_length)\n",
    "\n",
    "block_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYBBTllYgPEU"
   },
   "source": [
    "The following function will carry out the grouping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ymGys9gMgW4R"
   },
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "\n",
    "  # Concatenate all texts\n",
    "  concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "  total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "\n",
    "  # We make all input samples have the same length equal to block_size\n",
    "  # by dropping the small remainder, we could add padding if the model\n",
    "  # supported it instead of this drop\n",
    "  # You can customize this part to your needs.\n",
    "  total_length = (total_length // block_size) * block_size\n",
    "\n",
    "  # Split by chunks of max_len\n",
    "  result = {\n",
    "      k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "      for k, t in concatenated_examples.items()\n",
    "  }\n",
    "\n",
    "  # Create a new labels column as copy of the input_ids\n",
    "  result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tk4cR6W_IfIm"
   },
   "source": [
    "Note that in the former function's last step, we are creating a new ``labels`` column which is a copy of the ``input_ids`` one. This is because the goal of masked language modeling is to predict randomly masked tokens in the input batch, and by adding a labels column, we give the ground truth for our language model to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJTeeynUIgf0"
   },
   "outputs": [],
   "source": [
    "grouped_datasets = tokenized_datasets.map(group_texts, batched=True, batch_size=1000, num_proc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QGaIhCW_g4Ef"
   },
   "source": [
    "Here we are sending a batch of $1,000$ examples to be treated by the preprocessing function; this means that we will drop the remainder to make the concatenated tokenized texts a multiple of ``block_size`` every $1,000$ examples. You can adjust this behavior with a higher batch size, but note that this will make the processing slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wkIpBHav7R4"
   },
   "source": [
    "We can see that our datasets have changed: the samples now comprise blocks of block-size contiguous tokens that might span multiple of our original texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ikk1JB8qgtAr"
   },
   "outputs": [],
   "source": [
    "tokenizer.decode(grouped_datasets[\"train\"][1][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9MoD_YA1Tru"
   },
   "source": [
    "Having the dataset fully preprocessed, we have everything we need to carry out the fine-tuning of our model. Yet, there are not many differences from what we have been doing until now.\n",
    "\n",
    "In the next and last exercise of this notebook, your task will be to carry out the complete fine-tuning of a dataset of your choice (different from WikiTest) and its corresponding previous dataset preparation and preprocessing. Here are some instructions that you may find useful:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPJ-1Xyg2H5g"
   },
   "source": [
    "##### <font color='#2B4865'>**Some instructions on fine-tuning for text generation**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMne1_aOF6nf"
   },
   "source": [
    "You can mimic the training we carried out in the first notebook tutorial of Hugging Face Transformers, but note that:\n",
    "\n",
    "- The model that you need to use is one with a language modeling head (``AutoModelForCausalLM``).\n",
    "- There is no need for the ``compute_metrics`` function, as evaluation for text generation can be performed accurately based on the loss.\n",
    "- As DataCollator, use an instance of the class ``DataCollatorForLanguageModeling``. Since we are performing Causal Language modeling, you need to set the argument ``mlm`` to False. Before creating the collator, you will need to set the tokenizer's PAD token as the EOS token. The following code shows how to make the instantiation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tj7PWJZGzrjC"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0J48t1-0_iq"
   },
   "source": [
    "###### **Exercise 4.2**\n",
    "\n",
    "Your task here is to choose an autoregressive model from the Hub (try to use a different one from what we have been using in this tutorial) and a dataset to fine-tune for text generation (e.g., stories, code, etc.).\n",
    "\n",
    "As **dataset**, you **cannot use one provided in the Hub**. This means that you will need to find one from other resources (e.g., Kaggle) or construct it by yourself (e.g., you may want to generate movie dialogues following the Star Wars style; for this, you can web scrap the dialogues from IMSDb). If necessary, carry out the cleaning of the dataset.\n",
    "\n",
    "Once you have your dataset ready, proceed with its pre-processing as we have seen above, and fine-tune it following the specified guidelines. Choose the parameter that best fits your dataset.\n",
    "\n",
    "Once the model is trained, you can check its performance via the following code snippet:\n",
    "\n",
    "```python\n",
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "```\n",
    "\n",
    "After having the final fine-tuned model, create a pipeline and generate at least 10 different texts with it. For creating the pipeline, you will need to specify the model checkpoint, and the tokenizer and set the following argument:\n",
    "\n",
    "```python\n",
    "pad_token_id=tokenizer.eos_token_id\n",
    "```\n",
    "\n",
    "As the tokenizer for the pipeline, use that of the checkpoint you fine-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xWPqTkFxnKv6"
   },
   "outputs": [],
   "source": [
    "#<SOL>\n",
    "\n",
    "#</SOL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBUMDZPK0sNL"
   },
   "source": [
    "Once the training is completed, we can evaluate our model and get its perplexity on the validation set like this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "im-kXNf60pp3"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4rut1xKDrY-"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "chef = pipeline('text-generation', model= 'distilgpt2-finetuned-wikitext2/checkpoint-7000', tokenizer=checkpoint_name, pad_token_id=tokenizer.eos_token_id)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
