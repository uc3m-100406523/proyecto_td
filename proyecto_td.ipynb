{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaXcOsnwbb-r"
   },
   "source": [
    "Proyecto Final\n",
    "==============\n",
    "\n",
    "### Tratamiento de Datos\n",
    "### Máster de Ing. de Telecomunicación\n",
    "\n",
    "# Autores\n",
    "\n",
    "Juan Manuel Espinosa Moral ([100406523@alumnos.uc3m.es](mailto:100406523@alumnos.uc3m.es))\n",
    "\n",
    "José Manuel García Núñez ([100544621@alumnos.uc3m.es](mailto:100544621@alumnos.uc3m.es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTPdyX5-bb-w",
    "outputId": "d5c7ff8e-e06a-40a3-b9f4-ed2424de9f70",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Integración en Collab\n",
    "\n",
    "# Librerías de drive\n",
    "from google.colab import drive\n",
    "import os, sys\n",
    "\n",
    "# Montaje\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Directorio actual\n",
    "print(os.getcwd())\n",
    "\n",
    "# Cambio de directorio al compartido\n",
    "directory_path = \"/content/drive/MyDrive/Colab Notebooks/proyecto_td/\"  # path\n",
    "# If para crear el directorio en su path en caso de no existir\n",
    "if not os.path.exists(directory_path):\n",
    "  os.makedirs(directory_path)\n",
    "  print(f\"Directory created: {directory_path}\")\n",
    "\n",
    "os.chdir(directory_path) # switch de directorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpdJW6D0kTLy"
   },
   "source": [
    "# 1. Análisis de Variables de Entrada\n",
    "Carga del dataset: datos del archivo JSON.\n",
    "Categorías: las más frecuentes.\n",
    "Rating y visualizaciones.\n",
    "Análisis de correlación: categorias y variables de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hrXzyjOSl_ip",
    "outputId": "ba7ff4f7-f7e0-4512-8e80-992b81f230b8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargar el JSON\n",
    "df = pd.read_json(\"full_format_recipes.json\")\n",
    "\n",
    "# Explorar categories en cuanto a aparición\n",
    "category_counts = df['categories'].explode().value_counts()\n",
    "print(\"Categorías más frecuentes:\\n\", category_counts.head())\n",
    "\n",
    "# Top 10 categories\n",
    "category_counts.head(10).plot(kind='bar', title=\"Frecuencia de las categorías\")\n",
    "plt.ylabel(\"Número de recetas\")\n",
    "plt.show()\n",
    "\n",
    "# Analizar la relación entre categorías y ratings, filtrando por las más frecuentes\n",
    "df_exploded = df.explode('categories')  # Expandir listas de categorías\n",
    "df_exploded = df_exploded.reset_index(drop=True)  # Resetear el índice\n",
    "\n",
    "# Filtrar por las 10 categorías con mejor rating\n",
    "top_categories = category_counts.head(10).index.tolist()\n",
    "df_filtered = df_exploded[df_exploded['categories'].isin(top_categories)]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='categories', y='rating', data=df_filtered)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Distribución de ratings por categoría (Top 10)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "Dl_EY8F8Om-n",
    "outputId": "909b4b1c-70c4-461e-9fef-7aaf73b564a9"
   },
   "outputs": [],
   "source": [
    "# correlacion y heatmap de variables\n",
    "correlation = df[['fat', 'protein', 'calories', 'sodium', 'rating']].corr()\n",
    "\n",
    "print(correlation)\n",
    "\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 741
    },
    "id": "NDXZqe44k0e1",
    "outputId": "f577dfaa-142c-4e16-9ebc-f2b3ba0b7cfb"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Relación de ratings por categories\n",
    "df_exploded = df.explode('categories')  # Expandir listas de categorías\n",
    "df_exploded = df_exploded.reset_index(drop=True)\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='categories', y='rating', data=df_exploded)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Distribución de ratings por categoría\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "id": "U6qXseZePSY5",
    "outputId": "eaee88ff-8446-4210-a45a-cf58c11a12e7"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Analisis logarítmico relacionando fat y rating\n",
    "plt.scatter(df['fat'], df['rating'])\n",
    "plt.xscale('log')  # Apply logarithmic scale to x-axis\n",
    "plt.xlabel('Fat (grams) - Log Scale')\n",
    "plt.ylabel('Rating')\n",
    "plt.title('Fat vs. Rating (Log Scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKofPTudPSJw"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "TWYtY4TQQNjV",
    "outputId": "2e2acba8-202c-4891-8bdc-22f563c3ec9d"
   },
   "outputs": [],
   "source": [
    "# Relación entre rating y calories\n",
    "\n",
    "# se definen los tramos de calorias\n",
    "bins = [0, 200, 400, 600, 800, 1000, float('inf')]\n",
    "labels = ['0-200', '201-400', '401-600', '601-800', '801-1000', '1001+']\n",
    "\n",
    "# creación de una columna independiente con calorías\n",
    "df['calorie_bins'] = pd.cut(df['calories'], bins=bins, labels=labels)\n",
    "\n",
    "# creación de una medía de calorias en baase al rating\n",
    "average_ratings = df.groupby('calorie_bins')['rating'].mean()\n",
    "\n",
    "# graficación de resultados\n",
    "plt.bar(average_ratings.index, average_ratings.values)\n",
    "plt.xlabel('Calorie Range')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Average Rating vs. Calories')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "djjhEDQaQcpl",
    "outputId": "026c0530-296f-47df-e831-1520300560a8"
   },
   "outputs": [],
   "source": [
    "# se definen los tramos de grasa (gramos)\n",
    "bins_fat = [0, 10, 20, 30, 40, 50, float('inf')]\n",
    "labels_fat = ['0-10', '11-20', '21-30', '31-40', '41-50', '51+']\n",
    "\n",
    "# creación de una columna independiente con grasas\n",
    "df['fat_bins'] = pd.cut(df['fat'], bins=bins_fat, labels=labels_fat)\n",
    "\n",
    "# creación de una media de calorias en base a la cantidad de grasas\n",
    "average_ratings = df.groupby('fat_bins')['rating'].mean()\n",
    "\n",
    "# graficación de resultados\n",
    "plt.bar(average_ratings.index, average_ratings.values)\n",
    "plt.xlabel('Fat Range')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Average Rating vs. Fat')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "gVrn6Dl_QcMU",
    "outputId": "e5b17a63-93de-4201-a2e4-01050517a91d"
   },
   "outputs": [],
   "source": [
    "# se definen los tramos de proteina (gramos)\n",
    "bins_protein = [0, 10, 20, 30, 40, 50, float('inf')]\n",
    "labels_protein = ['0-10', '11-20', '21-30', '31-40', '41-50', '51+']\n",
    "\n",
    "# creación de una columna independiente con proteina\n",
    "df['protein_bins'] = pd.cut(df['protein'], bins=bins_protein, labels=labels_protein)\n",
    "\n",
    "# creación de una media de calorias en base a la cantidad de proteina\n",
    "average_ratings = df.groupby('protein_bins')['rating'].mean()\n",
    "\n",
    "# graficación de resultados\n",
    "plt.bar(average_ratings.index, average_ratings.values)\n",
    "plt.xlabel('Protein Range')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Average Rating vs. Protein')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oegguacSRuw_"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "OhObN5-FRx7h",
    "outputId": "7ef7af8e-7de1-47c3-d180-9ece673711b8"
   },
   "outputs": [],
   "source": [
    "# Esta parte no se si tiene mucho sentido mantener\n",
    "\n",
    "top_20_categories = df_exploded['categories'].value_counts().head(20).index.tolist()\n",
    "df_filtered = df_exploded[df_exploded['categories'].isin(top_20_categories)]\n",
    "\n",
    "variables = ['fat', 'protein', 'calories', 'sodium', 'rating']\n",
    "for variable in variables:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='categories', y=variable, data=df_filtered)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(f'{variable.capitalize()} Distribution by Top 20 Categories')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73Cyg0IoTfaE",
    "outputId": "5e041854-5413-4bd6-d155-2249fa1803e0"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "# cargado de spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# definición del método de procesado de texto del pipeline\n",
    "def preprocess_text(text, use_spacy=False):\n",
    "\n",
    "    # Si el texto es NaN o no es string/list, convertirlo a cadena vacía\n",
    "    if not isinstance(text, (str, list)):\n",
    "        text = ''\n",
    "\n",
    "    # si el texto es una lista, unir sus elementos en una sola cadena\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "\n",
    "    # pasar a minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Eliminar caracteres especiales y números\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    if use_spacy:\n",
    "        # Usar SpaCy para tokenización y lematización\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    else:\n",
    "        # Tokenizar el texto\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        # Eliminar stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        # Lematizar\n",
    "        lemmatizer = nltk.WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Unir tokens de nuevo en un solo string\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "# Manejar valores NaN en la columna 'categories'\n",
    "df['categories'] = df['categories'].fillna('')\n",
    "\n",
    "# Aplicar el pipeline al conjunto de datos\n",
    "df['processed_categories'] = df['categories'].apply(preprocess_text, use_spacy=True)\n",
    "\n",
    "# print de resultados\n",
    "print(df[['categories', 'processed_categories']].head())\n",
    "\n",
    "# Repetir el proceso con desc\n",
    "df['desc'] = df['desc'].fillna('')\n",
    "df['processed_desc'] = df['desc'].apply(preprocess_text, use_spacy=True)\n",
    "print(df[['desc', 'processed_desc']].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PxtNOxq-lPT_",
    "outputId": "ce460049-e87e-4f58-ac53-7b734bb92894"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Creación del objeto a vectorizar\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# conversión de datos para vectorizar\n",
    "tfidf_matrix1 = vectorizer.fit_transform(df['processed_categories'])\n",
    "# extracción de los nombres\n",
    "feature_names1 = vectorizer.get_feature_names_out()\n",
    "# creación del dataframe con los datos convertidos a arrays pasados por el método y los nombres extraidos\n",
    "tfidf_df1 = pd.DataFrame(tfidf_matrix1.toarray(), columns=feature_names1, index=df.index)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Repetición del proceso anterior para desc en lugar de categories\n",
    "vectorizer2 = TfidfVectorizer()  # Or use: vectorizer = TfidfVectorizer() if reusing\n",
    "tfidf_matrix2 = vectorizer2.fit_transform(df['processed_desc'])  # Or use: vectorizer.fit_transform if reusing\n",
    "feature_names2 = vectorizer2.get_feature_names_out()  # Or use: vectorizer.get_feature_names_out() if reusing\n",
    "tfidf_df2 = pd.DataFrame(tfidf_matrix2.toarray(), columns=feature_names2, index=df.index)\n",
    "\n",
    "# print de los resultados\n",
    "print(\"TF-IDF DataFrame for 'processed_categories':\")\n",
    "print(tfidf_df1)\n",
    "\n",
    "print(\"\\nTF-IDF DataFrame for 'processed_desc':\")\n",
    "print(tfidf_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Utijy9gCn2Vu",
    "outputId": "cc077fca-f13e-45ba-de18-186371dccc84"
   },
   "outputs": [],
   "source": [
    "!pip install nltk==3.8.1\n",
    "!pip install gensim\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "# Preparado de datos de texto\n",
    "sentences = [row.split() for row in df['processed_categories']]\n",
    "\n",
    "# Entrenamiento del modelo con las condiciones normales\n",
    "\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# 3. Obtén los vectores de palabras y promediados\n",
    "\n",
    "def get_category_vector(category_words):\n",
    "  \"\"\"Obtiene el vector promedio para una categoría.\"\"\"\n",
    "  vectors = [model.wv[word] for word in category_words if word in model.wv]\n",
    "  if vectors:\n",
    "    return np.mean(vectors, axis=0)\n",
    "  else:\n",
    "    return np.zeros(model.vector_size)  # Vector de ceros si no hay palabras en el vocabulario\n",
    "\n",
    "df['category_vector'] = df['processed_categories'].apply(lambda x: get_category_vector(x.split()))\n",
    "\n",
    "# Muestra los primeros 5 elementos del vector para las primeras 5 filas\n",
    "for index in range(5):\n",
    "  print(f\"Fila {index}: {df['category_vector'][index][:5]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mp7AwqkgVvBw",
    "outputId": "8d54d9d7-a201-447a-f820-f18cbd84fd11"
   },
   "outputs": [],
   "source": [
    "!pip install nltk==3.8.1\n",
    "!pip install gensim\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 1. Preparar datos de texto para 'categories'\n",
    "sentences1 = [row.split() for row in df['processed_categories']]\n",
    "\n",
    "# 2. Entrenar el modelo para 'categories'\n",
    "model1 = Word2Vec(sentences1, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# 3. Obtener vectores promediados para 'categories'\n",
    "def get_vector(text_words, model):\n",
    "  \"\"\"Obtiene el vector promedio para un texto.\"\"\"\n",
    "  vectors = [model.wv[word] for word in text_words if word in model.wv]\n",
    "  if vectors:\n",
    "    return np.mean(vectors, axis=0)\n",
    "  else:\n",
    "    return np.zeros(model.vector_size)  # Vector de ceros si no hay palabras en el vocabulario\n",
    "\n",
    "df['category_vector'] = df['processed_categories'].apply(lambda x: get_vector(x.split(), model1))\n",
    "\n",
    "# Muestra de 5 resultados\n",
    "print(\"Resultados para 'categories':\")\n",
    "for index in range(5):\n",
    "  print(f\"Fila {index}: {df['category_vector'][index][:5]}\")\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Repetir el proceso para desc\n",
    "sentences2 = [row.split() for row in df['processed_desc']]\n",
    "model2 = Word2Vec(sentences2, vector_size=100, window=5, min_count=5, workers=4)\n",
    "df['desc_vector'] = df['processed_desc'].apply(lambda x: get_vector(x.split(), model2))\n",
    "\n",
    "\n",
    "print(\"\\nResultados para 'desc':\")\n",
    "for index in range(5):\n",
    "  print(f\"Fila {index}: {df['desc_vector'][index][:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1UZRRQqq1SY"
   },
   "source": [
    "Testing de los metodos de vectorización TF-IDF y vec2sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_vLMNOVwq-gF",
    "outputId": "fcd90bab-b53c-414a-ef8e-efcecc0c6255"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA # Import PCA\n",
    "import numpy as np # Import numpy\n",
    "\n",
    "# Visualización de TF-IDF con Nube de Palabras\n",
    "word_weights = dict(zip(feature_names1, tfidf_matrix1.sum(axis=0).tolist()[0]))\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_weights)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Nube de Palabras TF-IDF\")\n",
    "plt.show()\n",
    "\n",
    "# Visualización de Word2Vec con PCA\n",
    "pca = PCA(n_components=2)\n",
    "vectors_2d = pca.fit_transform(df['category_vector'].to_list())\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1])\n",
    "plt.title(\"Visualización de vectores Word2Vec con PCA\")\n",
    "plt.xlabel(\"Componente Principal 1\")\n",
    "plt.ylabel(\"Componente Principal 2\")\n",
    "plt.show()\n",
    "\n",
    "# Visualización de Word2Vec con t-SNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "# Convert the list of vectors to a 2D NumPy array\n",
    "category_vectors = df['category_vector'].to_list()\n",
    "vectors_2d = tsne.fit_transform(np.array(category_vectors).reshape(len(category_vectors), -1)) # Reshape if necessary\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1])\n",
    "plt.title(\"Visualización de vectores Word2Vec con t-SNE\")\n",
    "plt.xlabel(\"Dimensión 1\")\n",
    "plt.ylabel(\"Dimensión 2\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
