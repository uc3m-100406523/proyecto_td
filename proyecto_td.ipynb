{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaXcOsnwbb-r"
   },
   "source": [
    "Proyecto Final\n",
    "==============\n",
    "\n",
    "### Tratamiento de Datos\n",
    "### Máster de Ing. de Telecomunicación\n",
    "\n",
    "## Autores\n",
    "\n",
    "Juan Manuel Espinosa Moral ([100406523@alumnos.uc3m.es](mailto:100406523@alumnos.uc3m.es))\n",
    "\n",
    "José Manuel García Núñez ([100544621@alumnos.uc3m.es](mailto:100544621@alumnos.uc3m.es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1536,
     "status": "ok",
     "timestamp": 1734094270863,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "hTPdyX5-bb-w",
    "outputId": "9f3c3cc6-df1c-4376-8d2a-fa301f8b85dd",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Integración en Collab\n",
    "\n",
    "# Librerías de drive\n",
    "from google.colab import drive\n",
    "import os, sys\n",
    "\n",
    "# Montaje\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Directorio actual\n",
    "print(os.getcwd())\n",
    "\n",
    "# Cambio de directorio al compartido\n",
    "directory_path = \"/content/drive/MyDrive/Colab Notebooks/proyecto_td/\"  # path\n",
    "# If para crear el directorio en su path en caso de no existir\n",
    "if not os.path.exists(directory_path):\n",
    "  os.makedirs(directory_path)\n",
    "  print(f\"Directory created: {directory_path}\")\n",
    "\n",
    "os.chdir(directory_path) # switch de directorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpdJW6D0kTLy"
   },
   "source": [
    "## 1. Análisis de Variables de Entrada\n",
    "\n",
    "- Carga del dataset: datos del archivo JSON.\n",
    "- Categorías: las más frecuentes.\n",
    "- Rating y visualizaciones.\n",
    "- Análisis de correlación: categorias y variables de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1734094270863,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "m7_RurhfC66S"
   },
   "outputs": [],
   "source": [
    "# Create directory to store results\n",
    "if not os.path.exists(\"data/section1/\"):\n",
    "  os.makedirs(\"data/section1/\")\n",
    "  print(f\"Directory created: {'data/section1/'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1770,
     "status": "ok",
     "timestamp": 1734094272632,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "hrXzyjOSl_ip",
    "outputId": "c3c48639-ff82-440e-f615-425140fef51a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargar el JSON\n",
    "if not os.path.exists(\"full_format_recipes.json\"):\n",
    "  df = pd.read_json(\"data/full_format_recipes.json\")\n",
    "else:\n",
    "  df = pd.read_json(\"full_format_recipes.json\")\n",
    "\n",
    "# Explorar categories en cuanto a aparición\n",
    "category_counts = df['categories'].explode().value_counts()\n",
    "print(\"Categorías más frecuentes:\\n\", category_counts.head())\n",
    "\n",
    "# Top 10 categories\n",
    "category_counts.head(10).plot(kind='bar', title=\"Frecuencia de las categorías\")\n",
    "plt.ylabel(\"Número de recetas\")\n",
    "plt.savefig(\"data/section1/no_recipes.png\")\n",
    "plt.show()\n",
    "\n",
    "# Analizar la relación entre categorías y ratings, filtrando por las más frecuentes\n",
    "df_exploded = df.explode('categories')  # Expandir listas de categorías\n",
    "df_exploded = df_exploded.reset_index(drop=True)  # Resetear el índice\n",
    "\n",
    "# Filtrar por las 10 categorías con mejor rating\n",
    "top_categories = category_counts.head(10).index.tolist()\n",
    "df_filtered = df_exploded[df_exploded['categories'].isin(top_categories)]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='categories', y='rating', data=df_filtered)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Distribución de ratings por categoría (Top 10)\")\n",
    "plt.savefig(\"data/section1/ratings_distribution_top10.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1734094272898,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "Dl_EY8F8Om-n",
    "outputId": "f0e598b7-df06-4df8-b0bd-5f5b02a8e315"
   },
   "outputs": [],
   "source": [
    "# correlacion y heatmap de variables\n",
    "correlation = df[['fat', 'protein', 'calories', 'sodium', 'rating']].corr()\n",
    "\n",
    "print(correlation)\n",
    "\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig(\"data/section1/corr.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "executionInfo": {
     "elapsed": 15613,
     "status": "ok",
     "timestamp": 1734094288509,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "NDXZqe44k0e1",
    "outputId": "d4c60b78-cd23-4443-ae40-20e10d5400d4"
   },
   "outputs": [],
   "source": [
    "# Relación de ratings por categories\n",
    "df_exploded = df.explode('categories')  # Expandir listas de categorías\n",
    "df_exploded = df_exploded.reset_index(drop=True)\n",
    "plt.figure(figsize=(120, 6))\n",
    "sns.boxplot(x='categories', y='rating', data=df_exploded)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Distribución de ratings por categoría\")\n",
    "plt.savefig(\"data/section1/ratings_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1734094288773,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "U6qXseZePSY5",
    "outputId": "f0c74a9b-b804-40db-bc85-21cccb67eb49"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Analisis logarítmico relacionando fat y rating\n",
    "plt.scatter(df['fat'], df['rating'])\n",
    "plt.xscale('log')  # Apply logarithmic scale to x-axis\n",
    "plt.xlabel('Fat (grams) - Log Scale')\n",
    "plt.ylabel('Rating')\n",
    "plt.title('Fat vs. Rating (Log Scale)')\n",
    "plt.savefig(\"data/section1/ratings_vs_calories_log.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1734094288773,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "TWYtY4TQQNjV",
    "outputId": "d79b671b-067e-48f2-8789-b938072aca9b"
   },
   "outputs": [],
   "source": [
    "# Relación entre rating y calories\n",
    "\n",
    "# se definen los tramos de calorias\n",
    "bins = [0, 200, 400, 600, 800, 1000, float('inf')]\n",
    "labels = ['0-200', '201-400', '401-600', '601-800', '801-1000', '1001+']\n",
    "\n",
    "# creación de una columna independiente con calorías\n",
    "df['calorie_bins'] = pd.cut(df['calories'], bins=bins, labels=labels)\n",
    "\n",
    "# creación de una medía de calorias en baase al rating\n",
    "average_ratings = df.groupby('calorie_bins')['rating'].mean()\n",
    "\n",
    "# graficación de resultados\n",
    "plt.bar(average_ratings.index, average_ratings.values)\n",
    "plt.xlabel('Calorie Range')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Average Rating vs. Calories')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/section1/ratings_vs_calories.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1734094289203,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "djjhEDQaQcpl",
    "outputId": "d48bf128-eace-4415-89ce-d116f78fcdde"
   },
   "outputs": [],
   "source": [
    "# se definen los tramos de grasa (gramos)\n",
    "bins_fat = [0, 10, 20, 30, 40, 50, float('inf')]\n",
    "labels_fat = ['0-10', '11-20', '21-30', '31-40', '41-50', '51+']\n",
    "\n",
    "# creación de una columna independiente con grasas\n",
    "df['fat_bins'] = pd.cut(df['fat'], bins=bins_fat, labels=labels_fat)\n",
    "\n",
    "# creación de una media de calorias en base a la cantidad de grasas\n",
    "average_ratings = df.groupby('fat_bins')['rating'].mean()\n",
    "\n",
    "# graficación de resultados\n",
    "plt.bar(average_ratings.index, average_ratings.values)\n",
    "plt.xlabel('Fat Range')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Average Rating vs. Fat')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/section1/ratings_vs_fat.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 345,
     "status": "ok",
     "timestamp": 1734094289546,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "gVrn6Dl_QcMU",
    "outputId": "c7a032c1-430c-45d1-b708-28b54a531d26"
   },
   "outputs": [],
   "source": [
    "# se definen los tramos de proteina (gramos)\n",
    "bins_protein = [0, 10, 20, 30, 40, 50, float('inf')]\n",
    "labels_protein = ['0-10', '11-20', '21-30', '31-40', '41-50', '51+']\n",
    "\n",
    "# creación de una columna independiente con proteina\n",
    "df['protein_bins'] = pd.cut(df['protein'], bins=bins_protein, labels=labels_protein)\n",
    "\n",
    "# creación de una media de calorias en base a la cantidad de proteina\n",
    "average_ratings = df.groupby('protein_bins')['rating'].mean()\n",
    "\n",
    "# graficación de resultados\n",
    "plt.bar(average_ratings.index, average_ratings.values)\n",
    "plt.xlabel('Protein Range')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Average Rating vs. Protein')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/section1/ratings_vs_protein.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhObN5-FRx7h"
   },
   "outputs": [],
   "source": [
    "# Esta parte no se si tiene mucho sentido mantener\n",
    "\n",
    "'''\n",
    "top_20_categories = df_exploded['categories'].value_counts().head(20).index.tolist()\n",
    "df_filtered = df_exploded[df_exploded['categories'].isin(top_20_categories)]\n",
    "\n",
    "variables = ['fat', 'protein', 'calories', 'sodium', 'rating']\n",
    "for variable in variables:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='categories', y=variable, data=df_filtered)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(f'{variable.capitalize()} Distribution by Top 20 Categories')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el resultado\n",
    "df.to_csv(\"data/section1/df_formatted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgLrM3Ptso30"
   },
   "source": [
    "## 2. Implementación de un *pipeline* para el preprocesado de los textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGGw0ptMC66T"
   },
   "outputs": [],
   "source": [
    "# Create directory to store results\n",
    "if not os.path.exists(\"data/section2/\"):\n",
    "  os.makedirs(\"data/section2/\")\n",
    "  print(f\"Directory created: {'data/section2/'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el DF de la sección anterior\n",
    "df = pd.read_csv(\"data/section1/df_formatted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73Cyg0IoTfaE"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "# cargado de spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# definición del método de procesado de texto del pipeline\n",
    "def preprocess_text(text, use_spacy=False):\n",
    "\n",
    "    # Si el texto es NaN o no es string/list, convertirlo a cadena vacía\n",
    "    if not isinstance(text, (str, list)):\n",
    "        text = ''\n",
    "\n",
    "    # si el texto es una lista, unir sus elementos en una sola cadena\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "\n",
    "    # pasar a minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Eliminar caracteres especiales y números\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    if use_spacy:\n",
    "        # Usar SpaCy para tokenización y lematización\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    else:\n",
    "        # Tokenizar el texto\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        # Eliminar stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        # Lematizar\n",
    "        lemmatizer = nltk.WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Unir tokens de nuevo en un solo string\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "# Para \"categories\"\n",
    "\n",
    "# Manejar valores NaN en la columna 'categories'\n",
    "df['categories'] = df['categories'].fillna('')\n",
    "\n",
    "# Aplicar el pipeline al conjunto de datos\n",
    "df['processed_categories'] = df['categories'].apply(preprocess_text, use_spacy=True)\n",
    "\n",
    "# print de resultados\n",
    "print(df[['categories', 'processed_categories']].head())\n",
    "\n",
    "# Para \"desc\"\n",
    "\n",
    "# Manejar valores NaN en la columna 'categories'\n",
    "df['desc'] = df['desc'].fillna('')\n",
    "\n",
    "# Aplicar el pipeline al conjunto de datos\n",
    "df['processed_desc'] = df['desc'].apply(preprocess_text, use_spacy=True)\n",
    "\n",
    "# print de resultados\n",
    "print(df[['desc', 'processed_desc']].head())\n",
    "\n",
    "# Guardamos el resultado\n",
    "df.to_csv(\"data/section2/df_proccessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNQE9hjWs5GH"
   },
   "source": [
    "## 3. Representación vectorial de los documentos mediante tres procedimientos diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CK66cTruC66U"
   },
   "outputs": [],
   "source": [
    "# Create directory to store results\n",
    "if not os.path.exists(\"data/section3/\"):\n",
    "  os.makedirs(\"data/section3/\")\n",
    "  print(f\"Directory created: {'data/section3/'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el DF de la sección anterior\n",
    "df = pd.read_csv(\"data/section2/df_proccessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hyRtRdxtAZP"
   },
   "source": [
    "### Procedimiento 1: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxtNOxq-lPT_"
   },
   "outputs": [],
   "source": [
    "# Bibliotecas\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Para \"categories\"\n",
    "\n",
    "# Creación del objeto a vectorizar\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# conversión de datos para vectorizar\n",
    "tfidf_matrix1 = vectorizer.fit_transform(df['processed_categories'].fillna(\"\").tolist())\n",
    "\n",
    "# extracción de los nombres\n",
    "feature_names1 = vectorizer.get_feature_names_out()\n",
    "\n",
    "# creación del dataframe con los datos convertidos a arrays pasados por el método y los nombres extraidos\n",
    "tfidf_df1 = pd.DataFrame(tfidf_matrix1.toarray(), columns=feature_names1, index=df.index)\n",
    "\n",
    "# Guardamos el resultado\n",
    "tfidf_df1.to_csv(\"data/section3/df_categories_with_tfidf.csv\", index=False)\n",
    "\n",
    "# print de los resultados\n",
    "print(\"TF-IDF DataFrame for 'processed_categories':\")\n",
    "#print(tfidf_df1)\n",
    "\n",
    "# Para \"desc\"\n",
    "\n",
    "# Creación del objeto a vectorizar\n",
    "vectorizer2 = TfidfVectorizer()  # Or use: vectorizer = TfidfVectorizer() if reusing\n",
    "\n",
    "# conversión de datos para vectorizar\n",
    "tfidf_matrix2 = vectorizer2.fit_transform(df['processed_desc'].fillna(\"\").tolist())  # Or use: vectorizer.fit_transform if reusing\n",
    "\n",
    "# extracción de los nombres\n",
    "feature_names2 = vectorizer2.get_feature_names_out()  # Or use: vectorizer.get_feature_names_out() if reusing\n",
    "\n",
    "# creación del dataframe con los datos convertidos a arrays pasados por el método y los nombres extraidos\n",
    "tfidf_df2 = pd.DataFrame(tfidf_matrix2.toarray(), columns=feature_names2, index=df.index)\n",
    "\n",
    "# print de los resultados\n",
    "print(\"\\nTF-IDF DataFrame for 'processed_desc':\")\n",
    "#print(tfidf_df2)\n",
    "\n",
    "# Guardamos el resultado\n",
    "tfidf_df2.to_csv(\"data/section3/df_desc_with_tfidf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY6hsf0wtHA4"
   },
   "source": [
    "### Procedimiento 2: *Word2Vec*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mp7AwqkgVvBw"
   },
   "outputs": [],
   "source": [
    "# Bibliotecas\n",
    "!pip install nltk==3.8.1\n",
    "!pip install gensim\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Método para obtener vectores promediados\n",
    "def get_vector(text_words, model):\n",
    "  \"\"\"Obtiene el vector promedio para un texto.\"\"\"\n",
    "  vectors = [model.wv[word] for word in text_words if word in model.wv]\n",
    "  if vectors:\n",
    "    return np.mean(vectors, axis=0)\n",
    "  else:\n",
    "    return np.zeros(model.vector_size)  # Vector de ceros si no hay palabras en el vocabulario\n",
    "\n",
    "# Word2Vec para \"categories\"\n",
    "\n",
    "# 1. Preparar datos de texto para 'categories'\n",
    "sentences1 = [row.split() for row in df['processed_categories'].fillna(\"\")]\n",
    "\n",
    "# 2. Entrenar el modelo para 'categories'\n",
    "model1 = Word2Vec(sentences1, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# 3. Obtener vectores promediados para 'categories'\n",
    "df['category_vector'] = df['processed_categories'].fillna(\"\").apply(lambda x: get_vector(x.split(), model1))\n",
    "\n",
    "# Muestra 5 resultados\n",
    "print(\"Resultados para 'categories':\")\n",
    "for index in range(5):\n",
    "  print(f\"Fila {index}: {df['category_vector'][index][:5]}\")\n",
    "\n",
    "# Word2Vec para \"desc\"\n",
    "\n",
    "# 1. Preparar datos de texto para 'categories'\n",
    "sentences2 = [row.split() for row in df['processed_desc'].fillna(\"\")]\n",
    "\n",
    "# 2. Entrenar el modelo para 'categories'\n",
    "model2 = Word2Vec(sentences2, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# 3. Obtener vectores promediados para 'categories'\n",
    "df['desc_vector'] = df['processed_desc'].fillna(\"\").apply(lambda x: get_vector(x.split(), model2))\n",
    "\n",
    "# Muestra 5 resultados\n",
    "print(\"\\nResultados para 'desc':\")\n",
    "for index in range(5):\n",
    "  print(f\"Fila {index}: {df['desc_vector'][index][:5]}\")\n",
    "\n",
    "# Guardamos el resultado\n",
    "df.to_csv(\"data/section3/df_with_word2vec.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1UZRRQqq1SY"
   },
   "source": [
    "### **DEBUG**: *Testing* de los metodos de vectorización TF-IDF y vec2sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vLMNOVwq-gF"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA # Import PCA\n",
    "import numpy as np # Import numpy\n",
    "\n",
    "# Visualización de TF-IDF con Nube de Palabras\n",
    "word_weights = dict(zip(feature_names1, tfidf_matrix1.sum(axis=0).tolist()[0]))\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_weights)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Nube de Palabras TF-IDF\")\n",
    "plt.savefig(\"data/section3/tf_idf_wordcloud.png\")\n",
    "plt.show()\n",
    "\n",
    "# Visualización de Word2Vec con PCA\n",
    "pca = PCA(n_components=2)\n",
    "vectors_2d = pca.fit_transform(df['category_vector'].to_list())\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1])\n",
    "plt.title(\"Visualización de vectores Word2Vec con PCA\")\n",
    "plt.xlabel(\"Componente Principal 1\")\n",
    "plt.ylabel(\"Componente Principal 2\")\n",
    "plt.savefig(\"data/section3/word2vec_pca.png\")\n",
    "plt.show()\n",
    "\n",
    "# Visualización de Word2Vec con t-SNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "# Convert the list of vectors to a 2D NumPy array\n",
    "category_vectors = df['category_vector'].to_list()\n",
    "vectors_2d = tsne.fit_transform(np.array(category_vectors).reshape(len(category_vectors), -1)) # Reshape if necessary\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1])\n",
    "plt.title(\"Visualización de vectores Word2Vec con t-SNE\")\n",
    "plt.xlabel(\"Dimensión 1\")\n",
    "plt.ylabel(\"Dimensión 2\")\n",
    "plt.savefig(\"data/section3/word2vec_tsne.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yv7nOIJ-tyYN"
   },
   "source": [
    "### Procedimiento 3: *Embeddings* contextuales calculados a partir de modelos basados en *transformers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sin gradientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xK7XNsbyt78r"
   },
   "outputs": [],
   "source": [
    "# Import de Pytorch y BERT\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Usar GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Reducir el tamaño del dataset a un 10%\n",
    "df_sampled = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Cargar el modelo y el tokenizer de BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Función para generar embeddings contextuales\n",
    "def generate_bert_embeddings(text):\n",
    "    # Tokenizar el texto\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Desactivar gradientes\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # extraer embeddings\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].squeeze(0).cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Aplicar BERT a las columnas y conversión de los embeddings en listas\n",
    "df_sampled['bert_embeddings_categories'] = df_sampled['processed_categories'].fillna(\"\").apply(lambda x: generate_bert_embeddings(x).tolist())\n",
    "df_sampled['bert_embeddings_desc'] = df_sampled['processed_desc'].fillna(\"\").apply(lambda x: generate_bert_embeddings(x).tolist())\n",
    "\n",
    "# Guardar el resultado en un archivo CSV (opcional)\n",
    "df_sampled.to_csv(\"data/section3/df_with_bert_embeddings.csv\", index=False)\n",
    "\n",
    "# visualizar algunos registros\n",
    "print(df_sampled[['processed_categories', 'bert_embeddings_categories']].head())\n",
    "print(df_sampled[['processed_desc', 'bert_embeddings_desc']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Con gradientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Usar GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Reducir el tamaño del dataset para pruebas (opcional)\n",
    "df_sampled = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Cargar el modelo y el tokenizer de BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Configurar el modelo en modo entrenamiento\n",
    "model.train()\n",
    "\n",
    "# Configurar un optimizador\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "def generate_bert_embeddings_with_gradients(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    cls_embeddings.retain_grad()\n",
    "    loss = cls_embeddings.norm()\n",
    "    loss.backward()\n",
    "    gradients = cls_embeddings.grad.detach().cpu().numpy()\n",
    "    return cls_embeddings.detach().cpu().numpy(), gradients\n",
    "\n",
    "# Aplicar BERT con gradientes a 'processed_categories'\n",
    "df_sampled['bert_embeddings_categories'] = df_sampled['processed_categories'].fillna(\"\").apply(\n",
    "    lambda x: generate_bert_embeddings_with_gradients(x)\n",
    ")\n",
    "\n",
    "# Aplicar BERT con gradientes a 'processed_desc'\n",
    "df_sampled['bert_embeddings_desc'] = df_sampled['processed_desc'].fillna(\"\").apply(\n",
    "    lambda x: generate_bert_embeddings_with_gradients(x)\n",
    ")\n",
    "\n",
    "# Guardar el resultado en un archivo CSV (opcional)\n",
    "df_sampled.to_csv(\"data/section3/df_with_bert_embeddings.csv\", index=False)\n",
    "\n",
    "# Visualizar los resultados\n",
    "print(df_sampled[['processed_categories', 'bert_embeddings_categories', 'processed_desc', 'bert_embeddings_desc']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento y evaluación de modelos de regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory to store results\n",
    "if not os.path.exists(\"data/section4/\"):\n",
    "  os.makedirs(\"data/section4/\")\n",
    "  print(f\"Directory created: {'data/section4/'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia 1: Redes neuronales utilizando PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 1: Bibliotecas y configuraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas de redes neuronales\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Biblioteca de optimización y función de pérdida\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Otras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Usar GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 2: Definir la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sencilla para nombres (usar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: Create a simple architecture for a recurrent neural network to analyze text using Torch nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])  # Get the last hidden state\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### De Josema (usar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del modelo\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, factor):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64*factor)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64*factor, 32*factor)\n",
    "        self.fc3 = nn.Linear(32*factor, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Específica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ratingNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # Connect the rest of layers\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])  # Get the last hidden state\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 3: Función de pérdida y optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función que devuelve las AUCs de tres elementos. Argumentos:\n",
    "#\toutputs (nx3): n el número de muestras en la base de datos\n",
    "#\tlabels (nx1)\n",
    "# La función devuelve un array de dos posiciones con los valores de las AUCs\n",
    "def computeAUCs(outputs, labels):\n",
    "\n",
    "    aucs = np.zeros((2,))\n",
    "\n",
    "    # Calculamos el AUC del primer elemento vs all\n",
    "    scores_mel = outputs[:, 1]\n",
    "    labels_mel = (labels == 1).astype(np.int)\n",
    "    aucs[0] = metrics.roc_auc_score(labels_mel, scores_mel)\n",
    "\n",
    "    # Calculamos el AUC del segundo elemento vs all\n",
    "    scores_sk = outputs[:, 2]\n",
    "    labels_sk = (labels == 2).astype(np.int)\n",
    "    aucs[1] = metrics.roc_auc_score(labels_sk, scores_sk)\n",
    "\n",
    "    return aucs\n",
    "\n",
    "# Función de pérdida\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "#criterion = nn.L1Loss()\n",
    "\n",
    "# Optimizador\n",
    "lr = 0.001\n",
    "momentum = 0.9\n",
    "#optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 4: Carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Para TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparamos los conjuntos\n",
    "# 1. Cargamos el DF de la sección de TF-IDF\n",
    "# 2. Combinar las representaciones TF-IDF generadas\n",
    "# 3. Verificar y manejar valores NaN en los datos\n",
    "# 4. Definir las etiquetas (target) y características (features)\n",
    "y = pd.read_csv(\"data/section2/df_proccessed.csv\")['rating'].fillna(0).values\n",
    "X = pd.concat([\n",
    "    pd.read_csv(\"data/section3/df_categories_with_tfidf.csv\"),\n",
    "    pd.read_csv(\"data/section3/df_desc_with_tfidf.csv\")\n",
    "    ], axis=1).fillna(0).values\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalización de los datos\n",
    "mean = X_train.mean(axis=0)\n",
    "std = X_train.std(axis=0)\n",
    "std[std == 0] = 1  # Evitar división por cero\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "# Eliminar posibles valores NaN o infinitos después de la normalización\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Para *Word2Vec*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el DF de la sección de Word2Vec\n",
    "df = pd.read_csv(\"data/section3/df_with_word2vec.csv\", usecols=['rating', 'category_vector'])\n",
    "\n",
    "vectorList = []\n",
    "for i, val in enumerate(df['category_vector'].values):\n",
    "  vectorList.append(np.array(val[1:-1].replace('\\n', ' ').split(), dtype=float))\n",
    "\n",
    "# Vectores de categorías y ratings\n",
    "X = np.stack(vectorList)\n",
    "y = df['rating'].values\n",
    "\n",
    "# Manejar valores NaN en los datos\n",
    "X = np.nan_to_num(X)  # Reemplazar NaN por 0\n",
    "y = np.nan_to_num(y)\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Para *Embeddings* contextuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el DF de la sección de contextual embeddings\n",
    "df = pd.read_csv(\"data/section3/df_with_bert_embeddings.csv\", usecols=['rating', 'category_vector'])\n",
    "\n",
    "vectorList = []\n",
    "for i, val in enumerate(df['category_vector'].values):\n",
    "  vectorList.append(np.array(val[1:-1].replace('\\n', ' ').split(), dtype=float))\n",
    "\n",
    "# Vectores de categorías y ratings\n",
    "X = np.stack(vectorList)\n",
    "y = df['rating'].values\n",
    "\n",
    "# Manejar valores NaN en los datos\n",
    "X = np.nan_to_num(X)  # Reemplazar NaN por 0\n",
    "y = np.nan_to_num(y)\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Genérico (ejecutar en todos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los datos a tensores de PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 5: Parametros del entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Para TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "batch_size = 32\n",
    "num_batches = len(X_train_tensor) // batch_size\n",
    "\n",
    "# Inicializar el modelo\n",
    "input_size = X_train.shape[1]\n",
    "model = RegressionModel(input_size, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Para *Word2Vec*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = len(X_train_tensor)\n",
    "num_batches = len(X_train_tensor) // batch_size\n",
    "\n",
    "# Inicializar el modelo\n",
    "input_size = X_train.shape[1]\n",
    "model = RegressionModel(input_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Para *Embeddings* contextuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = len(X_train_tensor)\n",
    "num_batches = len(X_train_tensor) // batch_size\n",
    "\n",
    "# Inicializar el modelo\n",
    "input_size = X_train.shape[1]\n",
    "model = RegressionModel(input_size, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 6: Ejecución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos la red al dispositivo que estemos usando (GPU)\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        inputs = X_train_tensor[start:end].to(device)\n",
    "        targets = y_train_tensor[start:end].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Verificar si la pérdida es NaN o infinita\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Stopping training at epoch {epoch + 1} due to NaN/infinite loss.\")\n",
    "            break\n",
    "\n",
    "        # Backward pass y optimización\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip de gradientes para evitar explosión\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Verificar si la pérdida es NaN o infinita\n",
    "    if torch.isnan(loss) or torch.isinf(loss):\n",
    "        print(f\"Stopping training at epoch {epoch + 1} due to NaN/infinite loss.\")\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/num_batches:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 7: Guardar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Para TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"data/section4/model_tfidf.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Para *Word2Vec*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"data/section4/model_word2vec.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Para *Embeddings* contextuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"data/section4/model_bert.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 8: Evaluar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Para TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo guardado\n",
    "input_size = X_train.shape[1]\n",
    "model = RegressionModel(input_size, 2)\n",
    "model.load_state_dict(torch.load(\"data/section4/model_tfidf.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Para *Word2Vec*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo guardado\n",
    "input_size = X_train.shape[1]\n",
    "model = RegressionModel(input_size, 1)\n",
    "model.load_state_dict(torch.load(\"data/section4/model_word2vec.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Para *Embeddings* contextuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo guardado\n",
    "input_size = X_train.shape[1]\n",
    "model = RegressionModel(input_size, 1)\n",
    "model.load_state_dict(torch.load(\"data/section4/model_bert.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo guardado\n",
    "#model = RegressionModel(input_size, 2)\n",
    "#model.load_state_dict(torch.load(\"data/section4/model_tfidf.pth\"))\n",
    "#model = RegressionModel(input_size, 1)\n",
    "#model.load_state_dict(torch.load(\"data/section4/model_word2vec.pth\"))\n",
    "input_size = X_train.shape[1]\n",
    "model = RegressionModel(input_size, 1)\n",
    "model.load_state_dict(torch.load(\"data/section4/model_bert.pth\"))\n",
    "\n",
    "# Evaluación del modelo\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_train = model(X_train_tensor).numpy()\n",
    "    y_pred_test = model(X_test_tensor).numpy()\n",
    "\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Train MSE: {train_mse:.4f}\")\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n",
    "print(f\"R^2: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia 2: otra técnica implementada en la librería *Scikit-learn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Terminar regresión\n",
    "w_ML = np.linalg.lstsq(Z, s, rcond=None)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: *Fine-tuning* de un modelo preentrenado con *Hugging Face*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory to store results\n",
    "if not os.path.exists(\"data/section5/\"):\n",
    "  os.makedirs(\"data/section5/\")\n",
    "  print(f\"Directory created: {'data/section5/'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets\n",
    "\n",
    "# Librerías adicionales para Hugging Face\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Otras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Usar GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el DF de la sección de contextual embeddings\n",
    "df = pd.read_csv(\"data/section2/df_proccessed.csv\", usecols=['processed_desc', 'rating'])\n",
    "\n",
    "'''\n",
    "vectorList = []\n",
    "for i, val in enumerate(df['desc_vector'].values):\n",
    "  vectorList.append(np.array(val[1:-1].replace('\\n', ' ').split(), dtype=float))\n",
    "'''\n",
    "\n",
    "# Dividir datos en train/val/test (usamos processed_desc como ejemplo)\n",
    "texts = df['processed_desc'].fillna(\"\").tolist()\n",
    "labels = df['rating'].fillna(0).tolist()  # Reemplaza con el nombre de tu columna objetivo\n",
    "\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(texts, labels, test_size=0.3, random_state=42)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# Tokenización con un modelo preentrenado (por ejemplo, BERT base uncased)\n",
    "model_name = \"bert-base-uncased\"  # Cambia si deseas otro modelo\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=128)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Preparar datasets para Hugging Face\n",
    "class RegressionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = RegressionDataset(train_encodings, train_labels)\n",
    "val_dataset = RegressionDataset(val_encodings, val_labels)\n",
    "test_dataset = RegressionDataset(test_encodings, test_labels)\n",
    "\n",
    "# Cargar modelo preentrenado con cabeza de regresión\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "model.to(device)\n",
    "\n",
    "# Configurar entrenamiento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "# Configurar entrenamiento\n",
    "#training_args = TrainingArguments(\n",
    "#    output_dir=\"./results\",\n",
    "#    num_train_epochs=1,  # Solo una época\n",
    "#    per_device_train_batch_size=4,  # Batch pequeño\n",
    "#    per_device_eval_batch_size=8,  # Batch de evaluación más grande\n",
    "#    warmup_steps=0,  # Sin pasos de calentamiento para pruebas rápidas\n",
    "#    weight_decay=0.01,  # Mantener ligero\n",
    "#    logging_dir=\"./logs\",\n",
    "#    logging_steps=50,  # Registrar cada 50 pasos\n",
    "#    evaluation_strategy=\"no\",  # No evaluar durante el entrenamiento\n",
    "#    save_strategy=\"no\",  # No guardar checkpoints\n",
    "#    disable_tqdm=True,  # Deshabilitar barra de progreso para mayor velocidad\n",
    "#    dataloader_num_workers=2,  # Reducir uso de threads\n",
    "#    fp16=True,  # Acelerar cálculos con precisión mixta (si tu hardware lo soporta)\n",
    "#)\n",
    "\n",
    "# Métricas de evaluación para regresión\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.squeeze()\n",
    "    mse = mean_squared_error(labels, preds)\n",
    "    mae = mean_absolute_error(labels, preds)\n",
    "    r2 = r2_score(labels, preds)\n",
    "    return {\"MSE\": mse, \"MAE\": mae, \"R2\": r2}\n",
    "\n",
    "# Entrenamiento con Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluación en el conjunto de prueba\n",
    "results = trainer.evaluate(test_dataset)\n",
    "print(\"Resultados del modelo fine-tuned:\", results)\n",
    "\n",
    "# Guardar el modelo ajustado\n",
    "model.save_pretrained(\"data/section5/fine-tuned-transformer-model\")\n",
    "tokenizer.save_pretrained(\"data/section5/fine-tuned-transformer-tokenizer\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
