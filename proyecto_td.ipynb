{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaXcOsnwbb-r"
   },
   "source": [
    "Proyecto Final\n",
    "==============\n",
    "\n",
    "### Tratamiento de Datos\n",
    "### Máster de Ing. de Telecomunicación\n",
    "\n",
    "## Autores\n",
    "\n",
    "Juan Manuel Espinosa Moral ([100406523@alumnos.uc3m.es](mailto:100406523@alumnos.uc3m.es))\n",
    "\n",
    "José Manuel García Núñez ([100544621@alumnos.uc3m.es](mailto:100544621@alumnos.uc3m.es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1536,
     "status": "ok",
     "timestamp": 1734094270863,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "hTPdyX5-bb-w",
    "outputId": "9f3c3cc6-df1c-4376-8d2a-fa301f8b85dd",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Integración en Collab\n",
    "\n",
    "# Librerías de drive\n",
    "from google.colab import drive\n",
    "import os, sys\n",
    "\n",
    "# Montaje\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Directorio actual\n",
    "print(os.getcwd())\n",
    "\n",
    "# Cambio de directorio al compartido\n",
    "directory_path = \"/content/drive/MyDrive/Colab Notebooks/proyecto_td/\"  # path\n",
    "# If para crear el directorio en su path en caso de no existir\n",
    "if not os.path.exists(directory_path):\n",
    "  os.makedirs(directory_path)\n",
    "  print(f\"Directory created: {directory_path}\")\n",
    "\n",
    "os.chdir(directory_path) # switch de directorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpdJW6D0kTLy"
   },
   "source": [
    "## 1. Análisis de Variables de Entrada\n",
    "\n",
    "- Carga del dataset: datos del archivo JSON.\n",
    "- Categorías: las más frecuentes.\n",
    "- Rating y visualizaciones.\n",
    "- Análisis de correlación: categorias y variables de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1734094270863,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "m7_RurhfC66S"
   },
   "outputs": [],
   "source": [
    "# Create directory to store results\n",
    "if not os.path.exists(\"data/section_1/\"):\n",
    "  os.makedirs(\"data/section_1/\")\n",
    "  print(f\"Directory created: {'data/section_1/'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1770,
     "status": "ok",
     "timestamp": 1734094272632,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "hrXzyjOSl_ip",
    "outputId": "c3c48639-ff82-440e-f615-425140fef51a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargar el JSON\n",
    "if not os.path.exists(\"full_format_recipes.json\"):\n",
    "  df = pd.read_json(\"data/full_format_recipes.json\")\n",
    "else:\n",
    "  df = pd.read_json(\"full_format_recipes.json\")\n",
    "\n",
    "# Explorar categories en cuanto a aparición\n",
    "category_counts = df['categories'].explode().value_counts()\n",
    "print(\"Categorías más frecuentes:\\n\", category_counts.head())\n",
    "\n",
    "# Top 10 categories\n",
    "category_counts.head(10).plot(kind='bar', title=\"Frecuencia de las categorías\")\n",
    "plt.ylabel(\"Número de recetas\")\n",
    "plt.savefig(\"data/section_1/no_recipes.png\")\n",
    "plt.show()\n",
    "\n",
    "# Analizar la relación entre categorías y ratings, filtrando por las más frecuentes\n",
    "df_exploded = df.explode('categories')  # Expandir listas de categorías\n",
    "df_exploded = df_exploded.reset_index(drop=True)  # Resetear el índice\n",
    "\n",
    "# Filtrar por las 10 categorías con mejor rating\n",
    "top_categories = category_counts.head(10).index.tolist()\n",
    "df_filtered = df_exploded[df_exploded['categories'].isin(top_categories)]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='categories', y='rating', data=df_filtered)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Distribución de ratings por categoría (Top 10)\")\n",
    "plt.savefig(\"data/section_1/ratings_distribution_top10.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1734094272898,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "Dl_EY8F8Om-n",
    "outputId": "f0e598b7-df06-4df8-b0bd-5f5b02a8e315"
   },
   "outputs": [],
   "source": [
    "# correlacion y heatmap de variables\n",
    "correlation = df[['fat', 'protein', 'calories', 'sodium', 'rating']].corr()\n",
    "\n",
    "print(correlation)\n",
    "\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig(\"data/section_1/corr.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "executionInfo": {
     "elapsed": 15613,
     "status": "ok",
     "timestamp": 1734094288509,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "NDXZqe44k0e1",
    "outputId": "d4c60b78-cd23-4443-ae40-20e10d5400d4"
   },
   "outputs": [],
   "source": [
    "# Relación de ratings por categories\n",
    "df_exploded = df.explode('categories')  # Expandir listas de categorías\n",
    "df_exploded = df_exploded.reset_index(drop=True)\n",
    "plt.figure(figsize=(60, 6))\n",
    "sns.boxplot(x='categories', y='rating', data=df_exploded)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Distribución de ratings por categoría\")\n",
    "plt.savefig(\"data/section_1/ratings_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1734094288773,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "U6qXseZePSY5",
    "outputId": "f0c74a9b-b804-40db-bc85-21cccb67eb49"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Analisis logarítmico relacionando fat y rating\n",
    "plt.scatter(df['fat'], df['rating'])\n",
    "plt.xscale('log')  # Apply logarithmic scale to x-axis\n",
    "plt.xlabel('Fat (grams) - Log Scale')\n",
    "plt.ylabel('Rating')\n",
    "plt.title('Fat vs. Rating (Log Scale)')\n",
    "plt.savefig(\"data/section_1/ratings_vs_calories_log.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1734094288773,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "TWYtY4TQQNjV",
    "outputId": "d79b671b-067e-48f2-8789-b938072aca9b"
   },
   "outputs": [],
   "source": [
    "# Relación entre rating y calories\n",
    "\n",
    "# se definen los tramos de calorias\n",
    "bins = [0, 200, 400, 600, 800, 1000, float('inf')]\n",
    "labels = ['0-200', '201-400', '401-600', '601-800', '801-1000', '1001+']\n",
    "\n",
    "# creación de una columna independiente con calorías\n",
    "df['calorie_bins'] = pd.cut(df['calories'], bins=bins, labels=labels)\n",
    "\n",
    "# creación de una medía de calorias en baase al rating\n",
    "average_ratings = df.groupby('calorie_bins')['rating'].mean()\n",
    "\n",
    "# graficación de resultados\n",
    "plt.bar(average_ratings.index, average_ratings.values)\n",
    "plt.xlabel('Calorie Range')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Average Rating vs. Calories')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/section_1/ratings_vs_calories.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1734094289203,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "djjhEDQaQcpl",
    "outputId": "d48bf128-eace-4415-89ce-d116f78fcdde"
   },
   "outputs": [],
   "source": [
    "# se definen los tramos de grasa (gramos)\n",
    "bins_fat = [0, 10, 20, 30, 40, 50, float('inf')]\n",
    "labels_fat = ['0-10', '11-20', '21-30', '31-40', '41-50', '51+']\n",
    "\n",
    "# creación de una columna independiente con grasas\n",
    "df['fat_bins'] = pd.cut(df['fat'], bins=bins_fat, labels=labels_fat)\n",
    "\n",
    "# creación de una media de calorias en base a la cantidad de grasas\n",
    "average_ratings = df.groupby('fat_bins')['rating'].mean()\n",
    "\n",
    "# graficación de resultados\n",
    "plt.bar(average_ratings.index, average_ratings.values)\n",
    "plt.xlabel('Fat Range')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Average Rating vs. Fat')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/section_1/ratings_vs_fat.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 345,
     "status": "ok",
     "timestamp": 1734094289546,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "gVrn6Dl_QcMU",
    "outputId": "c7a032c1-430c-45d1-b708-28b54a531d26"
   },
   "outputs": [],
   "source": [
    "# se definen los tramos de proteina (gramos)\n",
    "bins_protein = [0, 10, 20, 30, 40, 50, float('inf')]\n",
    "labels_protein = ['0-10', '11-20', '21-30', '31-40', '41-50', '51+']\n",
    "\n",
    "# creación de una columna independiente con proteina\n",
    "df['protein_bins'] = pd.cut(df['protein'], bins=bins_protein, labels=labels_protein)\n",
    "\n",
    "# creación de una media de calorias en base a la cantidad de proteina\n",
    "average_ratings = df.groupby('protein_bins')['rating'].mean()\n",
    "\n",
    "# graficación de resultados\n",
    "plt.bar(average_ratings.index, average_ratings.values)\n",
    "plt.xlabel('Protein Range')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Average Rating vs. Protein')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/section_1/ratings_vs_protein.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhObN5-FRx7h"
   },
   "outputs": [],
   "source": [
    "# Esta parte no se si tiene mucho sentido mantener\n",
    "\n",
    "top_20_categories = df_exploded['categories'].value_counts().head(20).index.tolist()\n",
    "df_filtered = df_exploded[df_exploded['categories'].isin(top_20_categories)]\n",
    "\n",
    "variables = ['fat', 'protein', 'calories', 'sodium', 'rating']\n",
    "for variable in variables:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='categories', y=variable, data=df_filtered)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(f'{variable.capitalize()} Distribution by Top 20 Categories')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgLrM3Ptso30"
   },
   "source": [
    "## 2. Implementación de un *pipeline* para el preprocesado de los textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGGw0ptMC66T"
   },
   "outputs": [],
   "source": [
    "# Create directory to store results\n",
    "if not os.path.exists(\"data/section_2/\"):\n",
    "  os.makedirs(\"data/section_2/\")\n",
    "  print(f\"Directory created: {'data/section_2/'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73Cyg0IoTfaE"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "# cargado de spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# definición del método de procesado de texto del pipeline\n",
    "def preprocess_text(text, use_spacy=False):\n",
    "\n",
    "    # Si el texto es NaN o no es string/list, convertirlo a cadena vacía\n",
    "    if not isinstance(text, (str, list)):\n",
    "        text = ''\n",
    "\n",
    "    # si el texto es una lista, unir sus elementos en una sola cadena\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "\n",
    "    # pasar a minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Eliminar caracteres especiales y números\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    if use_spacy:\n",
    "        # Usar SpaCy para tokenización y lematización\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    else:\n",
    "        # Tokenizar el texto\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        # Eliminar stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        # Lematizar\n",
    "        lemmatizer = nltk.WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Unir tokens de nuevo en un solo string\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "# Para \"categories\"\n",
    "\n",
    "# Manejar valores NaN en la columna 'categories'\n",
    "df['categories'] = df['categories'].fillna('')\n",
    "\n",
    "# Aplicar el pipeline al conjunto de datos\n",
    "df['processed_categories'] = df['categories'].apply(preprocess_text, use_spacy=True)\n",
    "\n",
    "# print de resultados\n",
    "print(df[['categories', 'processed_categories']].head())\n",
    "\n",
    "# Para \"desc\"\n",
    "\n",
    "# Manejar valores NaN en la columna 'categories'\n",
    "df['desc'] = df['desc'].fillna('')\n",
    "\n",
    "# Aplicar el pipeline al conjunto de datos\n",
    "df['processed_desc'] = df['desc'].apply(preprocess_text, use_spacy=True)\n",
    "\n",
    "# print de resultados\n",
    "print(df[['desc', 'processed_desc']].head())\n",
    "\n",
    "# Guardamos el resultado\n",
    "df.to_csv(\"data/section_2/df_proccessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNQE9hjWs5GH"
   },
   "source": [
    "## 3. Representación vectorial de los documentos mediante tres procedimientos diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CK66cTruC66U"
   },
   "outputs": [],
   "source": [
    "# Create directory to store results\n",
    "if not os.path.exists(\"data/section_3/\"):\n",
    "  os.makedirs(\"data/section_3/\")\n",
    "  print(f\"Directory created: {'data/section_3/'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hyRtRdxtAZP"
   },
   "source": [
    "### Procedimiento 1: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxtNOxq-lPT_"
   },
   "outputs": [],
   "source": [
    "# Bibliotecas\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Para \"categories\"\n",
    "\n",
    "# Creación del objeto a vectorizar\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# conversión de datos para vectorizar\n",
    "tfidf_matrix1 = vectorizer.fit_transform(df['processed_categories'])\n",
    "\n",
    "# extracción de los nombres\n",
    "feature_names1 = vectorizer.get_feature_names_out()\n",
    "\n",
    "# creación del dataframe con los datos convertidos a arrays pasados por el método y los nombres extraidos\n",
    "tfidf_df1 = pd.DataFrame(tfidf_matrix1.toarray(), columns=feature_names1, index=df.index)\n",
    "\n",
    "# Guardamos el resultado\n",
    "tfidf_df1.to_csv(\"data/section_3/df_categories_with_tfidf.csv\", index=False)\n",
    "\n",
    "# print de los resultados\n",
    "print(\"TF-IDF DataFrame for 'processed_categories':\")\n",
    "print(tfidf_df1)\n",
    "\n",
    "# Para \"desc\"\n",
    "\n",
    "# Creación del objeto a vectorizar\n",
    "vectorizer2 = TfidfVectorizer()  # Or use: vectorizer = TfidfVectorizer() if reusing\n",
    "\n",
    "# conversión de datos para vectorizar\n",
    "tfidf_matrix2 = vectorizer2.fit_transform(df['processed_desc'])  # Or use: vectorizer.fit_transform if reusing\n",
    "\n",
    "# extracción de los nombres\n",
    "feature_names2 = vectorizer2.get_feature_names_out()  # Or use: vectorizer.get_feature_names_out() if reusing\n",
    "\n",
    "# creación del dataframe con los datos convertidos a arrays pasados por el método y los nombres extraidos\n",
    "tfidf_df2 = pd.DataFrame(tfidf_matrix2.toarray(), columns=feature_names2, index=df.index)\n",
    "\n",
    "# print de los resultados\n",
    "print(\"\\nTF-IDF DataFrame for 'processed_desc':\")\n",
    "print(tfidf_df2)\n",
    "\n",
    "# Guardamos el resultado\n",
    "tfidf_df2.to_csv(\"data/section_3/df_desc_with_tfidf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY6hsf0wtHA4"
   },
   "source": [
    "### Procedimiento 2: *Word2Vec*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mp7AwqkgVvBw"
   },
   "outputs": [],
   "source": [
    "# Bibliotecas\n",
    "!pip install nltk==3.8.1\n",
    "!pip install gensim\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Método para obtener vectores promediados\n",
    "def get_vector(text_words, model):\n",
    "  \"\"\"Obtiene el vector promedio para un texto.\"\"\"\n",
    "  vectors = [model.wv[word] for word in text_words if word in model.wv]\n",
    "  if vectors:\n",
    "    return np.mean(vectors, axis=0)\n",
    "  else:\n",
    "    return np.zeros(model.vector_size)  # Vector de ceros si no hay palabras en el vocabulario\n",
    "\n",
    "# Word2Vec para \"categories\"\n",
    "\n",
    "# 1. Preparar datos de texto para 'categories'\n",
    "sentences1 = [row.split() for row in df['processed_categories']]\n",
    "\n",
    "# 2. Entrenar el modelo para 'categories'\n",
    "model1 = Word2Vec(sentences1, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# 3. Obtener vectores promediados para 'categories'\n",
    "df['category_vector'] = df['processed_categories'].apply(lambda x: get_vector(x.split(), model1))\n",
    "\n",
    "# Muestra 5 resultados\n",
    "print(\"Resultados para 'categories':\")\n",
    "for index in range(5):\n",
    "  print(f\"Fila {index}: {df['category_vector'][index][:5]}\")\n",
    "\n",
    "# Word2Vec para \"desc\"\n",
    "\n",
    "# 1. Preparar datos de texto para 'categories'\n",
    "sentences2 = [row.split() for row in df['processed_desc']]\n",
    "\n",
    "# 2. Entrenar el modelo para 'categories'\n",
    "model2 = Word2Vec(sentences2, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# 3. Obtener vectores promediados para 'categories'\n",
    "df['desc_vector'] = df['processed_desc'].apply(lambda x: get_vector(x.split(), model2))\n",
    "\n",
    "# Muestra 5 resultados\n",
    "print(\"\\nResultados para 'desc':\")\n",
    "for index in range(5):\n",
    "  print(f\"Fila {index}: {df['desc_vector'][index][:5]}\")\n",
    "\n",
    "# Guardamos el resultado\n",
    "df.to_csv(\"data/section_3/df_with_word2vec.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1UZRRQqq1SY"
   },
   "source": [
    "### **DEBUG**: *Testing* de los metodos de vectorización TF-IDF y vec2sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vLMNOVwq-gF"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA # Import PCA\n",
    "import numpy as np # Import numpy\n",
    "\n",
    "# Visualización de TF-IDF con Nube de Palabras\n",
    "word_weights = dict(zip(feature_names1, tfidf_matrix1.sum(axis=0).tolist()[0]))\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_weights)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Nube de Palabras TF-IDF\")\n",
    "plt.savefig(\"data/section_3/tf_idf_wordcloud.png\")\n",
    "plt.show()\n",
    "\n",
    "# Visualización de Word2Vec con PCA\n",
    "pca = PCA(n_components=2)\n",
    "vectors_2d = pca.fit_transform(df['category_vector'].to_list())\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1])\n",
    "plt.title(\"Visualización de vectores Word2Vec con PCA\")\n",
    "plt.xlabel(\"Componente Principal 1\")\n",
    "plt.ylabel(\"Componente Principal 2\")\n",
    "plt.savefig(\"data/section_3/word2vec_pca.png\")\n",
    "plt.show()\n",
    "\n",
    "# Visualización de Word2Vec con t-SNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "# Convert the list of vectors to a 2D NumPy array\n",
    "category_vectors = df['category_vector'].to_list()\n",
    "vectors_2d = tsne.fit_transform(np.array(category_vectors).reshape(len(category_vectors), -1)) # Reshape if necessary\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1])\n",
    "plt.title(\"Visualización de vectores Word2Vec con t-SNE\")\n",
    "plt.xlabel(\"Dimensión 1\")\n",
    "plt.ylabel(\"Dimensión 2\")\n",
    "plt.savefig(\"data/section_3/word2vec_tsne.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yv7nOIJ-tyYN"
   },
   "source": [
    "### Procedimiento 3: *Embeddings* contextuales calculados a partir de modelos basados en *transformers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xK7XNsbyt78r"
   },
   "outputs": [],
   "source": [
    "# Import de Pytorch y BERT\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Usar GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Reducir el tamaño del dataset a un 10%\n",
    "df_sampled = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Cargar el modelo y el tokenizer de BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Función para generar embeddings contextuales\n",
    "def generate_bert_embeddings(text):\n",
    "    # Tokenizar el texto\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Opción 1: Calcular los embeddings con gradientes activos\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Opción 2: desactivar gradientes para eficiencia\n",
    "    #with torch.no_grad():\n",
    "    #    outputs = model(**inputs)\n",
    "\n",
    "    # extraer embeddings\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].squeeze(0).cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Aplicar BERT a las columnas y conversión de los embeddings en listas\n",
    "df_sampled['bert_embeddings_categories'] = df_sampled['processed_categories'].apply(lambda x: generate_bert_embeddings(x).tolist())\n",
    "df_sampled['bert_embeddings_desc'] = df_sampled['processed_desc'].apply(lambda x: generate_bert_embeddings(x).tolist())\n",
    "\n",
    "# Guardar el resultado en un archivo CSV (opcional)\n",
    "df_sampled.to_csv(\"data/section_3/df_with_bert_embeddings.csv\", index=False)\n",
    "\n",
    "# visualizar algunos registros\n",
    "print(df_sampled[['processed_categories', 'bert_embeddings_categories']].head())\n",
    "print(df_sampled[['processed_desc', 'bert_embeddings_desc']].head())\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
