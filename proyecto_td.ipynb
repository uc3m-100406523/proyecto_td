{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gaXcOsnwbb-r"
   },
   "source": [
    "Proyecto Final\n",
    "==============\n",
    "\n",
    "### Tratamiento de Datos\n",
    "### Máster de Ing. de Telecomunicación\n",
    "\n",
    "## Autores\n",
    "\n",
    "Juan Manuel Espinosa Moral ([100406523@alumnos.uc3m.es](mailto:100406523@alumnos.uc3m.es))\n",
    "\n",
    "José Manuel García Núñez ([100544621@alumnos.uc3m.es](mailto:100544621@alumnos.uc3m.es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1536,
     "status": "ok",
     "timestamp": 1734094270863,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "hTPdyX5-bb-w",
    "outputId": "9f3c3cc6-df1c-4376-8d2a-fa301f8b85dd",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Integración en Collab\n",
    "\n",
    "# Librerías de drive\n",
    "from google.colab import drive\n",
    "import os, sys\n",
    "\n",
    "# Montaje\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Directorio actual\n",
    "print(os.getcwd())\n",
    "\n",
    "# Cambio de directorio al compartido\n",
    "directory_path = \"/content/drive/MyDrive/Colab Notebooks/proyecto_td/\"  # path\n",
    "# If para crear el directorio en su path en caso de no existir\n",
    "if not os.path.exists(directory_path):\n",
    "  os.makedirs(directory_path)\n",
    "  print(f\"Directory created: {directory_path}\")\n",
    "\n",
    "os.chdir(directory_path) # switch de directorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpdJW6D0kTLy"
   },
   "source": [
    "## 1. Análisis de Variables de Entrada\n",
    "\n",
    "- Carga del dataset: datos del archivo JSON.\n",
    "- Categorías: las más frecuentes.\n",
    "- Rating y visualizaciones.\n",
    "- Análisis de correlación: categorias y variables de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1734094270863,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "m7_RurhfC66S"
   },
   "outputs": [],
   "source": [
    "# Create directory to store results\n",
    "if not os.path.exists(\"data/section_1/\"):\n",
    "  os.makedirs(\"data/section_1/\")\n",
    "  print(f\"Directory created: {'data/section_1/'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1770,
     "status": "ok",
     "timestamp": 1734094272632,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "hrXzyjOSl_ip",
    "outputId": "c3c48639-ff82-440e-f615-425140fef51a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargar el JSON\n",
    "if not os.path.exists(\"full_format_recipes.json\"):\n",
    "  df = pd.read_json(\"data/full_format_recipes.json\")\n",
    "else:\n",
    "  df = pd.read_json(\"full_format_recipes.json\")\n",
    "\n",
    "# Explorar categories en cuanto a aparición\n",
    "category_counts = df['categories'].explode().value_counts()\n",
    "print(\"Categorías más frecuentes:\\n\", category_counts.head())\n",
    "\n",
    "# Top 10 categories\n",
    "category_counts.head(10).plot(kind='bar', title=\"Frecuencia de las categorías\")\n",
    "plt.ylabel(\"Número de recetas\")\n",
    "plt.savefig(\"data/section_1/no_recipes.png\")\n",
    "plt.show()\n",
    "\n",
    "# Analizar la relación entre categorías y ratings, filtrando por las más frecuentes\n",
    "df_exploded = df.explode('categories')  # Expandir listas de categorías\n",
    "df_exploded = df_exploded.reset_index(drop=True)  # Resetear el índice\n",
    "\n",
    "# Filtrar por las 10 categorías con mejor rating\n",
    "top_categories = category_counts.head(10).index.tolist()\n",
    "df_filtered = df_exploded[df_exploded['categories'].isin(top_categories)]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='categories', y='rating', data=df_filtered)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Distribución de ratings por categoría (Top 10)\")\n",
    "plt.savefig(\"data/section_1/ratings_distribution_top10.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1734094272898,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "Dl_EY8F8Om-n",
    "outputId": "f0e598b7-df06-4df8-b0bd-5f5b02a8e315"
   },
   "outputs": [],
   "source": [
    "# correlacion y heatmap de variables\n",
    "correlation = df[['fat', 'protein', 'calories', 'sodium', 'rating']].corr()\n",
    "\n",
    "print(correlation)\n",
    "\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.savefig(\"data/section_1/corr.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "executionInfo": {
     "elapsed": 15613,
     "status": "ok",
     "timestamp": 1734094288509,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "NDXZqe44k0e1",
    "outputId": "d4c60b78-cd23-4443-ae40-20e10d5400d4"
   },
   "outputs": [],
   "source": [
    "# Relación de ratings por categories\n",
    "df_exploded = df.explode('categories')  # Expandir listas de categorías\n",
    "df_exploded = df_exploded.reset_index(drop=True)\n",
    "plt.figure(figsize=(120, 6))\n",
    "sns.boxplot(x='categories', y='rating', data=df_exploded)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Distribución de ratings por categoría\")\n",
    "plt.savefig(\"data/section_1/ratings_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 274,
     "status": "ok",
     "timestamp": 1734094288773,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "U6qXseZePSY5",
    "outputId": "f0c74a9b-b804-40db-bc85-21cccb67eb49"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# Analisis logarítmico relacionando fat y rating\n",
    "plt.scatter(df['fat'], df['rating'])\n",
    "plt.xscale('log')  # Apply logarithmic scale to x-axis\n",
    "plt.xlabel('Fat (grams) - Log Scale')\n",
    "plt.ylabel('Rating')\n",
    "plt.title('Fat vs. Rating (Log Scale)')\n",
    "plt.savefig(\"data/section_1/ratings_vs_calories_log.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1734094288773,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "TWYtY4TQQNjV",
    "outputId": "d79b671b-067e-48f2-8789-b938072aca9b"
   },
   "outputs": [],
   "source": [
    "# Relación entre rating y calories\n",
    "\n",
    "# se definen los tramos de calorias\n",
    "bins = [0, 200, 400, 600, 800, 1000, float('inf')]\n",
    "labels = ['0-200', '201-400', '401-600', '601-800', '801-1000', '1001+']\n",
    "\n",
    "# creación de una columna independiente con calorías\n",
    "df['calorie_bins'] = pd.cut(df['calories'], bins=bins, labels=labels)\n",
    "\n",
    "# creación de una medía de calorias en baase al rating\n",
    "average_ratings = df.groupby('calorie_bins')['rating'].mean()\n",
    "\n",
    "# graficación de resultados\n",
    "plt.bar(average_ratings.index, average_ratings.values)\n",
    "plt.xlabel('Calorie Range')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Average Rating vs. Calories')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/section_1/ratings_vs_calories.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1734094289203,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "djjhEDQaQcpl",
    "outputId": "d48bf128-eace-4415-89ce-d116f78fcdde"
   },
   "outputs": [],
   "source": [
    "# se definen los tramos de grasa (gramos)\n",
    "bins_fat = [0, 10, 20, 30, 40, 50, float('inf')]\n",
    "labels_fat = ['0-10', '11-20', '21-30', '31-40', '41-50', '51+']\n",
    "\n",
    "# creación de una columna independiente con grasas\n",
    "df['fat_bins'] = pd.cut(df['fat'], bins=bins_fat, labels=labels_fat)\n",
    "\n",
    "# creación de una media de calorias en base a la cantidad de grasas\n",
    "average_ratings = df.groupby('fat_bins')['rating'].mean()\n",
    "\n",
    "# graficación de resultados\n",
    "plt.bar(average_ratings.index, average_ratings.values)\n",
    "plt.xlabel('Fat Range')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Average Rating vs. Fat')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/section_1/ratings_vs_fat.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 345,
     "status": "ok",
     "timestamp": 1734094289546,
     "user": {
      "displayName": "JUAN MANUEL ESPINOSA MORAL",
      "userId": "03725791287650385729"
     },
     "user_tz": -60
    },
    "id": "gVrn6Dl_QcMU",
    "outputId": "c7a032c1-430c-45d1-b708-28b54a531d26"
   },
   "outputs": [],
   "source": [
    "# se definen los tramos de proteina (gramos)\n",
    "bins_protein = [0, 10, 20, 30, 40, 50, float('inf')]\n",
    "labels_protein = ['0-10', '11-20', '21-30', '31-40', '41-50', '51+']\n",
    "\n",
    "# creación de una columna independiente con proteina\n",
    "df['protein_bins'] = pd.cut(df['protein'], bins=bins_protein, labels=labels_protein)\n",
    "\n",
    "# creación de una media de calorias en base a la cantidad de proteina\n",
    "average_ratings = df.groupby('protein_bins')['rating'].mean()\n",
    "\n",
    "# graficación de resultados\n",
    "plt.bar(average_ratings.index, average_ratings.values)\n",
    "plt.xlabel('Protein Range')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.title('Average Rating vs. Protein')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/section_1/ratings_vs_protein.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OhObN5-FRx7h"
   },
   "outputs": [],
   "source": [
    "# Esta parte no se si tiene mucho sentido mantener\n",
    "\n",
    "top_20_categories = df_exploded['categories'].value_counts().head(20).index.tolist()\n",
    "df_filtered = df_exploded[df_exploded['categories'].isin(top_20_categories)]\n",
    "\n",
    "variables = ['fat', 'protein', 'calories', 'sodium', 'rating']\n",
    "for variable in variables:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='categories', y=variable, data=df_filtered)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(f'{variable.capitalize()} Distribution by Top 20 Categories')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgLrM3Ptso30"
   },
   "source": [
    "## 2. Implementación de un *pipeline* para el preprocesado de los textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OGGw0ptMC66T"
   },
   "outputs": [],
   "source": [
    "# Create directory to store results\n",
    "if not os.path.exists(\"data/section_2/\"):\n",
    "  os.makedirs(\"data/section_2/\")\n",
    "  print(f\"Directory created: {'data/section_2/'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73Cyg0IoTfaE"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "\n",
    "# cargado de spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# definición del método de procesado de texto del pipeline\n",
    "def preprocess_text(text, use_spacy=False):\n",
    "\n",
    "    # Si el texto es NaN o no es string/list, convertirlo a cadena vacía\n",
    "    if not isinstance(text, (str, list)):\n",
    "        text = ''\n",
    "\n",
    "    # si el texto es una lista, unir sus elementos en una sola cadena\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "\n",
    "    # pasar a minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Eliminar caracteres especiales y números\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    if use_spacy:\n",
    "        # Usar SpaCy para tokenización y lematización\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    else:\n",
    "        # Tokenizar el texto\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        # Eliminar stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "        # Lematizar\n",
    "        lemmatizer = nltk.WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Unir tokens de nuevo en un solo string\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "# Para \"categories\"\n",
    "\n",
    "# Manejar valores NaN en la columna 'categories'\n",
    "df['categories'] = df['categories'].fillna('')\n",
    "\n",
    "# Aplicar el pipeline al conjunto de datos\n",
    "df['processed_categories'] = df['categories'].apply(preprocess_text, use_spacy=True)\n",
    "\n",
    "# print de resultados\n",
    "print(df[['categories', 'processed_categories']].head())\n",
    "\n",
    "# Para \"desc\"\n",
    "\n",
    "# Manejar valores NaN en la columna 'categories'\n",
    "df['desc'] = df['desc'].fillna('')\n",
    "\n",
    "# Aplicar el pipeline al conjunto de datos\n",
    "df['processed_desc'] = df['desc'].apply(preprocess_text, use_spacy=True)\n",
    "\n",
    "# print de resultados\n",
    "print(df[['desc', 'processed_desc']].head())\n",
    "\n",
    "# Guardamos el resultado\n",
    "df.to_csv(\"data/section_2/df_proccessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNQE9hjWs5GH"
   },
   "source": [
    "## 3. Representación vectorial de los documentos mediante tres procedimientos diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CK66cTruC66U"
   },
   "outputs": [],
   "source": [
    "# Create directory to store results\n",
    "if not os.path.exists(\"data/section_3/\"):\n",
    "  os.makedirs(\"data/section_3/\")\n",
    "  print(f\"Directory created: {'data/section_3/'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hyRtRdxtAZP"
   },
   "source": [
    "### Procedimiento 1: TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PxtNOxq-lPT_"
   },
   "outputs": [],
   "source": [
    "# Bibliotecas\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Para \"categories\"\n",
    "\n",
    "# Creación del objeto a vectorizar\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# conversión de datos para vectorizar\n",
    "tfidf_matrix1 = vectorizer.fit_transform(df['processed_categories'])\n",
    "\n",
    "# extracción de los nombres\n",
    "feature_names1 = vectorizer.get_feature_names_out()\n",
    "\n",
    "# creación del dataframe con los datos convertidos a arrays pasados por el método y los nombres extraidos\n",
    "tfidf_df1 = pd.DataFrame(tfidf_matrix1.toarray(), columns=feature_names1, index=df.index)\n",
    "\n",
    "# Guardamos el resultado\n",
    "tfidf_df1.to_csv(\"data/section_3/df_categories_with_tfidf.csv\", index=False)\n",
    "\n",
    "# print de los resultados\n",
    "print(\"TF-IDF DataFrame for 'processed_categories':\")\n",
    "print(tfidf_df1)\n",
    "\n",
    "# Para \"desc\"\n",
    "\n",
    "# Creación del objeto a vectorizar\n",
    "vectorizer2 = TfidfVectorizer()  # Or use: vectorizer = TfidfVectorizer() if reusing\n",
    "\n",
    "# conversión de datos para vectorizar\n",
    "tfidf_matrix2 = vectorizer2.fit_transform(df['processed_desc'])  # Or use: vectorizer.fit_transform if reusing\n",
    "\n",
    "# extracción de los nombres\n",
    "feature_names2 = vectorizer2.get_feature_names_out()  # Or use: vectorizer.get_feature_names_out() if reusing\n",
    "\n",
    "# creación del dataframe con los datos convertidos a arrays pasados por el método y los nombres extraidos\n",
    "tfidf_df2 = pd.DataFrame(tfidf_matrix2.toarray(), columns=feature_names2, index=df.index)\n",
    "\n",
    "# print de los resultados\n",
    "print(\"\\nTF-IDF DataFrame for 'processed_desc':\")\n",
    "print(tfidf_df2)\n",
    "\n",
    "# Guardamos el resultado\n",
    "tfidf_df2.to_csv(\"data/section_3/df_desc_with_tfidf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vY6hsf0wtHA4"
   },
   "source": [
    "### Procedimiento 2: *Word2Vec*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mp7AwqkgVvBw"
   },
   "outputs": [],
   "source": [
    "# Bibliotecas\n",
    "!pip install nltk==3.8.1\n",
    "!pip install gensim\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Método para obtener vectores promediados\n",
    "def get_vector(text_words, model):\n",
    "  \"\"\"Obtiene el vector promedio para un texto.\"\"\"\n",
    "  vectors = [model.wv[word] for word in text_words if word in model.wv]\n",
    "  if vectors:\n",
    "    return np.mean(vectors, axis=0)\n",
    "  else:\n",
    "    return np.zeros(model.vector_size)  # Vector de ceros si no hay palabras en el vocabulario\n",
    "\n",
    "# Word2Vec para \"categories\"\n",
    "\n",
    "# 1. Preparar datos de texto para 'categories'\n",
    "sentences1 = [row.split() for row in df['processed_categories']]\n",
    "\n",
    "# 2. Entrenar el modelo para 'categories'\n",
    "model1 = Word2Vec(sentences1, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# 3. Obtener vectores promediados para 'categories'\n",
    "df['category_vector'] = df['processed_categories'].apply(lambda x: get_vector(x.split(), model1))\n",
    "\n",
    "# Muestra 5 resultados\n",
    "print(\"Resultados para 'categories':\")\n",
    "for index in range(5):\n",
    "  print(f\"Fila {index}: {df['category_vector'][index][:5]}\")\n",
    "\n",
    "# Word2Vec para \"desc\"\n",
    "\n",
    "# 1. Preparar datos de texto para 'categories'\n",
    "sentences2 = [row.split() for row in df['processed_desc']]\n",
    "\n",
    "# 2. Entrenar el modelo para 'categories'\n",
    "model2 = Word2Vec(sentences2, vector_size=100, window=5, min_count=5, workers=4)\n",
    "\n",
    "# 3. Obtener vectores promediados para 'categories'\n",
    "df['desc_vector'] = df['processed_desc'].apply(lambda x: get_vector(x.split(), model2))\n",
    "\n",
    "# Muestra 5 resultados\n",
    "print(\"\\nResultados para 'desc':\")\n",
    "for index in range(5):\n",
    "  print(f\"Fila {index}: {df['desc_vector'][index][:5]}\")\n",
    "\n",
    "# Guardamos el resultado\n",
    "df.to_csv(\"data/section_3/df_with_word2vec.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1UZRRQqq1SY"
   },
   "source": [
    "### **DEBUG**: *Testing* de los metodos de vectorización TF-IDF y vec2sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vLMNOVwq-gF"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA # Import PCA\n",
    "import numpy as np # Import numpy\n",
    "\n",
    "# Visualización de TF-IDF con Nube de Palabras\n",
    "word_weights = dict(zip(feature_names1, tfidf_matrix1.sum(axis=0).tolist()[0]))\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_weights)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(\"Nube de Palabras TF-IDF\")\n",
    "plt.savefig(\"data/section_3/tf_idf_wordcloud.png\")\n",
    "plt.show()\n",
    "\n",
    "# Visualización de Word2Vec con PCA\n",
    "pca = PCA(n_components=2)\n",
    "vectors_2d = pca.fit_transform(df['category_vector'].to_list())\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1])\n",
    "plt.title(\"Visualización de vectores Word2Vec con PCA\")\n",
    "plt.xlabel(\"Componente Principal 1\")\n",
    "plt.ylabel(\"Componente Principal 2\")\n",
    "plt.savefig(\"data/section_3/word2vec_pca.png\")\n",
    "plt.show()\n",
    "\n",
    "# Visualización de Word2Vec con t-SNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "# Convert the list of vectors to a 2D NumPy array\n",
    "category_vectors = df['category_vector'].to_list()\n",
    "vectors_2d = tsne.fit_transform(np.array(category_vectors).reshape(len(category_vectors), -1)) # Reshape if necessary\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1])\n",
    "plt.title(\"Visualización de vectores Word2Vec con t-SNE\")\n",
    "plt.xlabel(\"Dimensión 1\")\n",
    "plt.ylabel(\"Dimensión 2\")\n",
    "plt.savefig(\"data/section_3/word2vec_tsne.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yv7nOIJ-tyYN"
   },
   "source": [
    "### Procedimiento 3: *Embeddings* contextuales calculados a partir de modelos basados en *transformers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sin gradientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xK7XNsbyt78r"
   },
   "outputs": [],
   "source": [
    "# Import de Pytorch y BERT\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Usar GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Reducir el tamaño del dataset a un 10%\n",
    "df_sampled = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Cargar el modelo y el tokenizer de BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Función para generar embeddings contextuales\n",
    "def generate_bert_embeddings(text):\n",
    "    # Tokenizar el texto\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    # Desactivar gradientes\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # extraer embeddings\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].squeeze(0).cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Aplicar BERT a las columnas y conversión de los embeddings en listas\n",
    "df_sampled['bert_embeddings_categories'] = df_sampled['processed_categories'].apply(lambda x: generate_bert_embeddings(x).tolist())\n",
    "df_sampled['bert_embeddings_desc'] = df_sampled['processed_desc'].apply(lambda x: generate_bert_embeddings(x).tolist())\n",
    "\n",
    "# Guardar el resultado en un archivo CSV (opcional)\n",
    "df_sampled.to_csv(\"data/section_3/df_with_bert_embeddings.csv\", index=False)\n",
    "\n",
    "# visualizar algunos registros\n",
    "print(df_sampled[['processed_categories', 'bert_embeddings_categories']].head())\n",
    "print(df_sampled[['processed_desc', 'bert_embeddings_desc']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Con gradientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Usar GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Reducir el tamaño del dataset para pruebas (opcional)\n",
    "df_sampled = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Cargar el modelo y el tokenizer de BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Configurar el modelo en modo entrenamiento\n",
    "model.train()\n",
    "\n",
    "# Configurar un optimizador\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "def generate_bert_embeddings_with_gradients(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "    cls_embeddings.retain_grad()\n",
    "    loss = cls_embeddings.norm()\n",
    "    loss.backward()\n",
    "    gradients = cls_embeddings.grad.detach().cpu().numpy()\n",
    "    return cls_embeddings.detach().cpu().numpy(), gradients\n",
    "\n",
    "# Aplicar BERT con gradientes a 'processed_categories'\n",
    "df_sampled['bert_embeddings_categories'] = df_sampled['processed_categories'].apply(\n",
    "    lambda x: generate_bert_embeddings_with_gradients(x)\n",
    ")\n",
    "\n",
    "# Aplicar BERT con gradientes a 'processed_desc'\n",
    "df_sampled['bert_embeddings_desc'] = df_sampled['processed_desc'].apply(\n",
    "    lambda x: generate_bert_embeddings_with_gradients(x)\n",
    ")\n",
    "\n",
    "# Guardar el resultado en un archivo CSV (opcional)\n",
    "df_sampled.to_csv(\"data/section_3/df_with_bert_embeddings.csv\", index=False)\n",
    "\n",
    "# Visualizar los resultados\n",
    "print(df_sampled[['processed_categories', 'bert_embeddings_categories', 'processed_desc', 'bert_embeddings_desc']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento y evaluación de modelos de regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia 1: Redes neuronales utilizando PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 1: Bibliotecas y configuraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas de redes neuronales\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Biblioteca de optimización y función de pérdida\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Usar GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 2: Definir la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: La siguiente clase se basa en LeNet -> Reformular para texto\n",
    "class Net(nn.Module):\n",
    "\n",
    "    # En el inicializador vamos a especificar los bloques de cómputo que tienen parámetros a definir, es decir, las capas que serán necesarias. Los definimos de forma independiente (sin unir), pues de momento son bloques aislados y no forman una red.\n",
    "    def __init__(self):\n",
    "\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Capa convolucional:\n",
    "        #\t- Canales in = 3 (la imagen de entrada tiene 3 canales RGB)\n",
    "        #\t- 6 canales out\n",
    "        #\t- Filtro de tamaño 5x5\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "\n",
    "        # Capa de Maxpooling con tamaño 2x2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Capa convolucional:\n",
    "        #\t- Canales in = 6 (de la capa anterior)\n",
    "        #\t- 16 canales out\n",
    "        #\t- Filtro de tamaño 5x5\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "\n",
    "        # Capa completamente conectada (y = Wx + b):\n",
    "        #\t- Canales in = 16 * 5 * 5 (16 capa anterior, 5x5 es la dimensión de la imagen que llega a esta capa)\n",
    "        #\t- Canales out = 120\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "\n",
    "        # Capa completamente conectada (y = Wx + b)\n",
    "        #\t- Canales in = 120 (capa anterior)\n",
    "        #\t- Canales out = 84\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "\n",
    "        # Capa completamente conectada (y = Wx + b)\n",
    "        #\t- Canales in = 84 (capa anterior)\n",
    "        #\t- Canales out = 10 (tenemos 10 dígitos a clasificar)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    # En \"forward\" definimos la estructura de la red a través de su grafo computacional. Es donde conectamos los bloques antes definidos y metemos otros más simples.\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Entrada -> conv1 -> activación relu -> Max pooling sobre una ventana (2, 2) -> x\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "\n",
    "        # x -> conv2 -> activación relu -> Max pooling sobre una ventana (2, 2) -> x\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Cambiamos la forma del tensor para vectorizarlo (16x6x6 -> 120) -> x\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "\n",
    "        # x -> fc1 -> relu -> x\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        # x -> fc2 -> relu -> x\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        # fc3\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Invocamos el constructor de la red (método init())\n",
    "net = Net()\n",
    "\n",
    "# Pasamos la red al dispositivo que estemos usando (GPU)\n",
    "net.to(device)\n",
    "\n",
    "# Información de la red\n",
    "\n",
    "## Obtenemos la lista\n",
    "params = list(net.parameters())\n",
    "\n",
    "## Número de parámetros\n",
    "print(\"Número de parámetros de la red {:d}\".format(len(params)))\n",
    "\n",
    "## Tamaño de los parámetros:\n",
    "for param in params:\n",
    "    print(param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento con los valores de TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 5.1733\n",
      "Epoch [2/5], Loss: 1.6628\n",
      "Epoch [3/5], Loss: 1.2356\n",
      "Epoch [4/5], Loss: 1.1136\n",
      "Epoch [5/5], Loss: 0.9447\n",
      "MSE: 2.5845\n",
      "R^2: -0.4950\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Preparación de los datos\n",
    "\n",
    "# Cargar los datos\n",
    "tfidf_df1 = pd.read_csv(\"data/section_3/df_categories_with_tfidf.csv\")\n",
    "tfidf_df2 = pd.read_csv(\"data/section_3/df_desc_with_tfidf.csv\")\n",
    "\n",
    "# Combinar las representaciones TF-IDF generadas\n",
    "data_combined = pd.concat([tfidf_df1, tfidf_df2], axis=1)\n",
    "\n",
    "# Verificar y manejar valores NaN en los datos\n",
    "data_combined = data_combined.fillna(0)  # Rellenar NaN con 0\n",
    "y = df['rating'].fillna(0).values  # Rellenar NaN en el target con 0\n",
    "\n",
    "# Definir las etiquetas (target) y características (features)\n",
    "X = data_combined.values\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalización de los datos\n",
    "mean = X_train.mean(axis=0)\n",
    "std = X_train.std(axis=0)\n",
    "std[std == 0] = 1  # Evitar división por cero\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "# Eliminar posibles valores NaN o infinitos después de la normalización\n",
    "X_train = np.nan_to_num(X_train)\n",
    "X_test = np.nan_to_num(X_test)\n",
    "\n",
    "# Convertir los datos a tensores de PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Definición del modelo\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instanciar el modelo\n",
    "input_size = X_train.shape[1]\n",
    "model = RegressionModel(input_size)\n",
    "\n",
    "# Definir la función de pérdida y el optimizador\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "epochs = 50\n",
    "batch_size = 32\n",
    "num_batches = len(X_train_tensor) // batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        inputs = X_train_tensor[start:end]\n",
    "        targets = y_train_tensor[start:end]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass y optimización\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip de gradientes para evitar explosión\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/num_batches:.4f}\")\n",
    "\n",
    "# Evaluación del modelo\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_test_tensor).numpy()\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"R^2: {r2:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento con los valores de *Word2Vec*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 15.3623\n",
      "Epoch [20/100], Loss: 15.1992\n",
      "Epoch [30/100], Loss: 15.0344\n",
      "Epoch [40/100], Loss: 14.8671\n",
      "Epoch [50/100], Loss: 14.6948\n",
      "Epoch [60/100], Loss: 14.5119\n",
      "Epoch [70/100], Loss: 14.3104\n",
      "Epoch [80/100], Loss: 14.0764\n",
      "Epoch [90/100], Loss: 13.8234\n",
      "Epoch [100/100], Loss: 13.5528\n",
      "Train MSE: 13.5245\n",
      "Test MSE: 13.5902\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Cargar los datos\n",
    "df = pd.read_csv(\"data/section_3/df_with_word2vec.csv\")\n",
    "\n",
    "# Convertir los vectores de categorías y ratings a tensores de PyTorch\n",
    "X = np.stack(df['category_vector'].values)\n",
    "y = df['rating'].values\n",
    "\n",
    "# Manejar valores NaN en los datos\n",
    "X = np.nan_to_num(X)  # Reemplazar NaN por 0\n",
    "y = np.nan_to_num(y)\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convertir los datos a tensores de PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Definir la red neuronal\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Inicializar el modelo, la función de pérdida y el optimizador\n",
    "input_size = X_train.shape[1]\n",
    "model = RegressionModel(input_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Reducir la tasa de aprendizaje\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "    # Verificar si la pérdida es NaN o infinita\n",
    "    if torch.isnan(loss) or torch.isinf(loss):\n",
    "        print(f\"Stopping training at epoch {epoch + 1} due to NaN/infinite loss.\")\n",
    "        break\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluación del modelo\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_train = model(X_train_tensor).numpy()\n",
    "    y_pred_test = model(X_test_tensor).numpy()\n",
    "\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Train MSE: {train_mse:.4f}\")\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento con los valores de *embeddings* contextuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 15.3623\n",
      "Epoch [20/100], Loss: 15.1992\n",
      "Epoch [30/100], Loss: 15.0344\n",
      "Epoch [40/100], Loss: 14.8671\n",
      "Epoch [50/100], Loss: 14.6948\n",
      "Epoch [60/100], Loss: 14.5119\n",
      "Epoch [70/100], Loss: 14.3104\n",
      "Epoch [80/100], Loss: 14.0764\n",
      "Epoch [90/100], Loss: 13.8234\n",
      "Epoch [100/100], Loss: 13.5528\n",
      "Train MSE: 13.5245\n",
      "Test MSE: 13.5902\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Cargar los datos\n",
    "df = pd.read_csv(\"data/section_3/df_with_bert_embeddings.csv\")\n",
    "\n",
    "# Convertir los vectores de categorías y ratings a tensores de PyTorch\n",
    "X = np.stack(df['category_vector'].values)\n",
    "y = df['rating'].values\n",
    "\n",
    "# Manejar valores NaN en los datos\n",
    "X = np.nan_to_num(X)  # Reemplazar NaN por 0\n",
    "y = np.nan_to_num(y)\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convertir los datos a tensores de PyTorch\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Definir la red neuronal\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Inicializar el modelo, la función de pérdida y el optimizador\n",
    "input_size = X_train.shape[1]\n",
    "model = RegressionModel(input_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Reducir la tasa de aprendizaje\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "    # Verificar si la pérdida es NaN o infinita\n",
    "    if torch.isnan(loss) or torch.isinf(loss):\n",
    "        print(f\"Stopping training at epoch {epoch + 1} due to NaN/infinite loss.\")\n",
    "        break\n",
    "\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluación del modelo\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_train = model(X_train_tensor).numpy()\n",
    "    y_pred_test = model(X_test_tensor).numpy()\n",
    "\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Train MSE: {train_mse:.4f}\")\n",
    "print(f\"Test MSE: {test_mse:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
